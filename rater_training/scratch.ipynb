{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92505d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForImageTextRetrieval\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-large-coco\")\n",
    "model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-large-coco\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"qkv\", \"query\", \"key\", \"value\", \"dense\", \"projection\", \"fc1\", \"fc2\", \"text_proj\", \"visual_proj\", \"position_embeddings\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e96fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor(images=Image.new('RGB', (94, 34)), text=\"A cat\", return_tensors=\"pt\").pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.cuda()\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "train_ds = load_dataset(\"zai-org/VisionRewardDB-Image\", split='train[:40000]')\n",
    "test_ds = load_dataset(\"zai-org/VisionRewardDB-Image\", split='train[40000:]')\n",
    "\n",
    "import io, math, random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"rules.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "df['Dimension'] = df['Dimension'].ffill()\n",
    "\n",
    "df['dim_key'] = df['Dimension'].apply(lambda x: re.search(r'\\((.*?)\\)', x).group(1) if re.search(r'\\((.*?)\\)', x) else x)\n",
    "\n",
    "guide = {\n",
    "    dim_key: {\n",
    "        int(row['Score']): row['Option'] + \": \" +str(row['Description']).strip()\n",
    "        for _, row in group.iterrows()\n",
    "    }\n",
    "    for dim_key, group in df.groupby('dim_key')\n",
    "}\n",
    "\n",
    "dims = {k: v for k, v in guide.items() if k not in [\"unsafe type\", \"hands\", \"face\", \"body\", \"safety\", \"lighting aesthetic\", \"symmetry\"]}.keys()\n",
    "dims = list(dims)\n",
    "dim_min = {i:min(guide[i].keys()) for i in guide.keys()}\n",
    "\n",
    "# %%\n",
    "\n",
    "import json\n",
    "with open(\"prompts.json\", \"r\") as f:\n",
    "    prompt_dict = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d58562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_data(sample):\n",
    "    images = []\n",
    "    dims_selected = []\n",
    "    # print(len(sample[\"image\"]), len(sample[\"annotation\"]))\n",
    "    for image in range(len(sample['image'])):\n",
    "        images.append(sample['image'][image])\n",
    "        try:\n",
    "            if random.random()>0.5:\n",
    "                # sample a dim with score>=0 \n",
    "                dims_selected.append(random.choice(list([i for i in dims if sample['annotation'][image][i]>=0])))\n",
    "            else:\n",
    "                # sample a dim with score<0\n",
    "                dims_selected.append(random.choice(list([i for i in dims if sample['annotation'][image][i]<0])))\n",
    "        except IndexError:\n",
    "            dims_selected.append(random.choice(dims))\n",
    "            \n",
    "\n",
    "    prompts = [prompt_dict[dim] for i, dim in enumerate(dims_selected)]\n",
    "    images = list(sample['image'])\n",
    "    n_images = len(images)\n",
    "    n_prompts = len(prompts) \n",
    "    inputs = processor(images=images, text=prompts, return_tensors=\"pt\", padding=True)\n",
    "    answers = [1 if i[dim]<0 else (0.5 if i[dim]==0 else 0) for i, dim in zip(sample[\"annotation\"], dims_selected)]\n",
    "    labels = torch.tensor(answers)\n",
    "    inputs['labels'] = labels\n",
    "    inputs['dim'] = [dims.index(dim) for dim in dims_selected]\n",
    "    inputs['n_images'] = [n_images] * len(inputs['input_ids'])\n",
    "    return {\n",
    "        'pixel_values': inputs['pixel_values'],\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': inputs['labels'], \n",
    "        'dims': dims_selected,\n",
    "        'n_images': inputs['n_images'],\n",
    "        # \"annotation\": [i[dim] for i, dim in zip(sample[\"annotation\"], dims_selected)],\n",
    "    } \n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "train_ds = train_ds.with_transform(format_data)\n",
    "test_ds = test_ds.with_transform(format_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d31cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds[0:7][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "class Rater(PreTrainedModel):\n",
    "    def __init__(self, backbone):\n",
    "      super().__init__(PretrainedConfig())\n",
    "      self.backbone = backbone\n",
    "      self.head = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask, n_images, labels=None):\n",
    "      n_images = n_images[0]\n",
    "      outputs = self.backbone(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask)\n",
    "      itm_scores = self.head(outputs.question_embeds[:,0,:]).squeeze(-1)\n",
    "\n",
    "      if labels is not None:\n",
    "        assert itm_scores.shape == labels.shape, f\"{itm_scores.shape} {labels.shape}\"\n",
    "        assert itm_scores.shape[0] == n_images\n",
    "        bce_loss = torch.nn.functional.binary_cross_entropy_with_logits(itm_scores, labels)\n",
    "        mae_loss = torch.nn.functional.l1_loss(torch.sigmoid(itm_scores), labels)\n",
    "        loss = bce_loss + mae_loss\n",
    "\n",
    "        try: \n",
    "          wandb.log({\"bce_loss\": bce_loss, \"acc\": ((itm_scores>0) == (labels>0.5)).float().mean(), \"mae_loss\": mae_loss})\n",
    "        except:\n",
    "          pass\n",
    "        outputs['loss'] = loss\n",
    "\n",
    "      return outputs\n",
    "\n",
    "my_rater = Rater(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79635da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_rater = my_rater.cpu()\n",
    "# with torch.no_grad():\n",
    "#     my_rater(**train_ds[0:2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"BLIP-Reward-Long\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.001,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    max_grad_norm=1.0,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=min(os.cpu_count(), 16),\n",
    "    fp16=True,\n",
    "    warmup_ratio=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # lr_scheduler_kwargs={\"num_decay_steps\": 500},\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"qkv\", \"query\", \"key\", \"value\", \"dense\", \"projection\"],\n",
    "    modules_to_save=[\"head\"] \n",
    ")\n",
    "my_rater = get_peft_model(my_rater, lora_config)  \n",
    "my_rater = my_rater.to(\"cuda\")\n",
    "my_rater.print_trainable_parameters()\n",
    "from transformers import Trainer \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=my_rater,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    processing_class=processor,\n",
    ")\n",
    "\n",
    "# %%\n",
    "trainer.train() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"weathon/aas_benchmark\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bbc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "train_ds = load_dataset(\"zai-org/VisionRewardDB-Image\", split='train[:40000]')\n",
    "test_ds = load_dataset(\"zai-org/VisionRewardDB-Image\", split='train[40000:]')\n",
    "\n",
    "import io, math, random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"rules.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "df['Dimension'] = df['Dimension'].ffill()\n",
    "\n",
    "df['dim_key'] = df['Dimension'].apply(lambda x: re.search(r'\\((.*?)\\)', x).group(1) if re.search(r'\\((.*?)\\)', x) else x)\n",
    "\n",
    "guide = {\n",
    "    dim_key: {\n",
    "        int(row['Score']): row['Option'] + \": \" +str(row['Description']).strip()\n",
    "        for _, row in group.iterrows()\n",
    "    }\n",
    "    for dim_key, group in df.groupby('dim_key')\n",
    "}\n",
    "\n",
    "dims = {k: v for k, v in guide.items() if k not in [\"unsafe type\", \"hands\", \"face\", \"body\", \"safety\", \"lighting aesthetic\", \"symmetry\"]}.keys()\n",
    "dims = list(dims)\n",
    "dim_min = {i:min(guide[i].keys()) for i in guide.keys()}\n",
    "\n",
    "# %%\n",
    "\n",
    "import json\n",
    "with open(\"prompts.json\", \"r\") as f:\n",
    "    prompt_dict = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = \" \".join(list(dataset['prompt_original']))\n",
    "all_distorted_prompts = \" \".join(list(dataset['prompt_distorted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "text = all_distorted_prompts\n",
    "\n",
    "text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    " \n",
    "wordcloud = WordCloud(width=256, height=412, background_color='white').generate(text)\n",
    "Image.fromarray(np.array(wordcloud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f838815",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_interest = {}\n",
    "for dim in guide.keys():\n",
    "    if dim not in [\"unsafe type\", \"hands\", \"face\", \"body\", \"safety\", \"lighting aesthetic\", \"symmetry\"]:\n",
    "        index_of_interest[dim] = dims.index(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_rater(sample):\n",
    "    original = np.array(sample[\"rater\"][\"original\"][\"scores\"])\n",
    "    distorted = np.array(sample[\"rater\"][\"distorted\"][\"scores\"])\n",
    "    sample['delta_rater'] = sum(distorted - original)\n",
    "    sample['rater_dp'] = sum(distorted)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hpsv3_reward(sample):\n",
    "    original = sample[\"hpsv3_reward\"][\"hpsv3_oidp\"][0]\n",
    "    distorted = sample[\"hpsv3_reward\"][\"hpsv3_didp\"][0]\n",
    "    sample['delta_hpsv3_reward'] = distorted - original\n",
    "    sample['hpsv3_reward_dp'] = distorted\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rater_selected(sample):\n",
    "    dims = json.loads(sample[\"selected_dims\"])\n",
    "    idx = np.array([index_of_interest[dim] for dim in dims])\n",
    "    original = np.array(sample[\"rater\"][\"original\"][\"scores\"])\n",
    "    original = np.take(original, idx)\n",
    "    distorted = np.array(sample[\"rater\"][\"distorted\"][\"scores\"])\n",
    "    distorted = np.take(distorted, idx)\n",
    "    sample['delta_rater_selected'] = sum(distorted - original)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.remove_columns(['image_original', 'image_distorted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49619ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(get_delta_rater)\n",
    "dataset = dataset.map(get_hpsv3_reward)\n",
    "dataset = dataset.map(get_rater_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab\n",
    "from scipy import stats\n",
    "x = dataset['delta_rater_selected']\n",
    "y = dataset['delta_hpsv3_reward']\n",
    "df = pd.DataFrame({'x': x, 'y': y}) \n",
    "# df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "pylab.scatter(df['x'], df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40197916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc96c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 30\n",
    "# idx = np.where((df['x'] < 0) & (df['y'] > 0))[0][idx]\n",
    "# print(dataset[idx][\"hpsv3_reward\"][\"hpsv3_oidp\"][0], dataset[idx][\"hpsv3_reward\"][\"hpsv3_didp\"][0], dataset[idx][\"delta_hpsv3_reward\"])\n",
    "# img_original = dataset[idx][\"image_original\"].convert(\"RGB\")\n",
    "# img_distorted = dataset[idx][\"image_distorted\"].convert(\"RGB\")\n",
    "# new_img = Image.new(\"RGB\", (img_original.width + img_distorted.width, img_original.height))\n",
    "# new_img.paste(img_original, (0, 0))\n",
    "# new_img.paste(img_distorted, (img_original.width, 0))\n",
    "# display(new_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7810106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "pearson_corr, _ = pearsonr(x, y)\n",
    "spearman_corr, _ = spearmanr(x, y)\n",
    "print(f\"Pearson correlation: {pearson_corr}\")\n",
    "print(f\"Spearman correlation: {spearman_corr}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40022264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kendall tau\n",
    "from scipy.stats import kendalltau\n",
    "kendall_corr, _ = kendalltau(np.array(dataset['delta_rater']).mean(-1), dataset['hpsv3_reward'])\n",
    "print(f\"Kendall tau correlation: {kendall_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d99248",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_index = np.where(np.array(dataset['delta_rater']) < 0)[0]\n",
    "failed_index = np.where(np.array(dataset['delta_rater']) >= 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b80c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.hist(np.array(dataset['hpsv3_reward'])[success_index], alpha=0.5, label='successful cases')\n",
    "pylab.hist(np.array(dataset['hpsv3_reward'])[failed_index], alpha=0.5, label='failed cases') \n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d23bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dataset['delta_rater'])\n",
    "Y = np.array(dataset['hpsv3_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb2eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "key = os.environ[\"DEEPNFRA_API_KEY\"]\n",
    "\n",
    "openai = OpenAI(\n",
    "    api_key=key,\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n",
    "\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=\"Qwen/Qwen3-VL-30B-A3B-Thinking\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What is in this image?\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"weathon/aas_benchmark\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b57250",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"rater\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5cad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idxs = 10, 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8feb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a10c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ca87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 107 980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7356c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "idx = 980\n",
    "print(\"llm_selected\", dataset[\"llm_selected\"][idx])\n",
    "print(\"HPSv2 selected\", int(dataset[idx][\"hpsv2_reward\"][\"hpsv2_didp\"] > dataset[idx][\"hpsv2_reward\"][\"hpsv2_oidp\"]))\n",
    "print(\"HPSv3 selected\", int(dataset[idx][\"hpsv3_reward\"][\"hpsv3_didp\"][0] > dataset[idx][\"hpsv3_reward\"][\"hpsv3_oidp\"][0]))\n",
    "print(\"image_reward selected\", int(dataset[idx][\"image_reward\"][\"image_reward_didp\"] > dataset[idx][\"image_reward\"][\"image_reward_oidp\"]))\n",
    "# print(\"rater_selected\", int(sum(dataset[idx][\"rater\"][\"distorted\"][\"scores\"]) < sum(dataset[idx][\"rater\"][\"original\"][\"scores\"])))\n",
    "pylab.subplot(1, 2, 1)\n",
    "pylab.imshow(dataset[\"image_original\"][idx])\n",
    "pylab.axis(\"off\")\n",
    "pylab.subplot(1, 2, 2)\n",
    "pylab.imshow(dataset[\"image_distorted\"][idx])\n",
    "pylab.axis(\"off\")\n",
    "pylab.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c77cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db921ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_score = [i.tolist() for i in blip_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc35ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde6f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"weathon/aas_benchmark\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [10, 980, 109, 208, 906, 394]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.where(np.array(dataset[\"llm_selected\"])!=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db03f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab473fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import json\n",
    "idx = 27\n",
    "print(\"llm_selected\", dataset[\"llm_selected\"][idx])\n",
    "print(\"HPSv2 selected\", int(dataset[idx][\"hpsv2_reward\"][\"hpsv2_didp\"] > dataset[idx][\"hpsv2_reward\"][\"hpsv2_oidp\"]))\n",
    "print(\"HPSv3 selected\", int(dataset[idx][\"hpsv3_reward\"][\"hpsv3_didp\"][0] > dataset[idx][\"hpsv3_reward\"][\"hpsv3_oidp\"][0]))\n",
    "print(\"image_reward selected\", int(dataset[idx][\"image_reward\"][\"image_reward_didp\"] > dataset[idx][\"image_reward\"][\"image_reward_oidp\"]))\n",
    "print(\"blip selected\", dataset[\"blip_selected\"][idx]) \n",
    "print(\"Effect\", \", \".join(json.loads(dataset[\"selected_dims\"][idx])))\n",
    "print(\"Effect\", dataset[\"prompt_distorted\"][idx])\n",
    "print(\"Effect\", dataset[\"selected_dims\"][idx])\n",
    "# print(\"rater_selected\", int(sum(dataset[idx][\"rater\"][\"distorted\"][\"scores\"]) < sum(dataset[idx][\"rater\"][\"original\"][\"scores\"])))\n",
    "pylab.subplot(1, 2, 1)\n",
    "pylab.imshow(dataset[\"image_original\"][idx])\n",
    "pylab.axis(\"off\")\n",
    "pylab.subplot(1, 2, 2)\n",
    "pylab.imshow(dataset[\"image_distorted\"][idx])\n",
    "pylab.axis(\"off\")\n",
    "pylab.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no_image = dataset.remove_columns(['image_original', 'image_distorted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39da8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpsv3_selected = []\n",
    "for i in tqdm.tqdm(range(len(dataset_no_image))):\n",
    "    hpsv3_selected.append(int(dataset_no_image[i][\"hpsv3_reward\"][\"hpsv3_didp\"][0] > dataset_no_image[i][\"hpsv3_reward\"][\"hpsv3_oidp\"][0]))\n",
    "\n",
    "hpsv2_selected = []\n",
    "for i in tqdm.tqdm(range(len(dataset_no_image))):\n",
    "    hpsv2_selected.append(int(dataset_no_image[i][\"hpsv2_reward\"][\"hpsv2_didp\"] > dataset_no_image[i][\"hpsv2_reward\"][\"hpsv2_oidp\"]))\n",
    "\n",
    "image_reward_selected = []\n",
    "for i in tqdm.tqdm(range(len(dataset_no_image))):\n",
    "    image_reward_selected.append(int(dataset_no_image[i][\"image_reward\"][\"image_reward_didp\"] > dataset_no_image[i][\"image_reward\"][\"image_reward_oidp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tie\n",
    "\n",
    "hpsv3_selected = np.array(hpsv3_selected)\n",
    "hpsv2_selected = np.array(hpsv2_selected)\n",
    "image_reward_selected = np.array(image_reward_selected)\n",
    "\n",
    "filtered_indices = np.array(dataset_no_image[\"llm_selected\"]) != -1\n",
    "hpsv3_selected = hpsv3_selected[filtered_indices]\n",
    "hpsv2_selected = hpsv2_selected[filtered_indices]\n",
    "image_reward_selected = image_reward_selected[filtered_indices]\n",
    "llm_selected = np.array(dataset_no_image[\"llm_selected\"])[filtered_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7271e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kappa\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "kappa_hpsv3 = cohen_kappa_score(llm_selected, hpsv3_selected)\n",
    "kappa_hpsv3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a78588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 \n",
    "from sklearn.metrics import f1_score\n",
    "f1_hpsv3 = f1_score(llm_selected, hpsv3_selected, average='micro')\n",
    "f1_hpsv3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy_score(llm_selected, image_reward_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9bf9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no_image = dataset.remove_columns(['image_original', 'image_distorted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0003d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset\n",
    "counter = {}\n",
    "models = []\n",
    "for idx, m in zip(ds[\"index\"], ds[\"model\"]):\n",
    "    if m == \"sd3_medium_grpo\":\n",
    "        key = (idx, m)\n",
    "        counter[key] = counter.get(key, 0) + 1\n",
    "        if counter[key] == 1:\n",
    "            models.append(m)\n",
    "        else:\n",
    "            models.append(m + \"_\" + str(counter[key]))\n",
    "    else:\n",
    "        models.append(m)\n",
    "ds = ds.remove_columns(\"model\").add_column(\"model\", models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ds[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.push_to_hub(\"weathon/aas_benchmark_2\", private=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4cd66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.array(dataset[\"model\"])==\"sd3_medium_grpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = ['flux_dev', 'dance_flux', 'grpo_flux', 'flux_krea', 'stable_diffusion_xl',\n",
    "       'playground', 'stable_diffusion_3.5_medium', 'sd3_medium_grpo', 'sd3_medium_grpo_geneval', 'nano-banana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b93adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_mapping = {\n",
    "    'flux_dev': \"Flux Dev\",\n",
    "    'dance_flux': \"Dance Flux\", \n",
    "    'grpo_flux': \"PrefFlux\",\n",
    "    'flux_krea': \"Flux Krea\",\n",
    "    'stable_diffusion_xl': \"SDXL\",\n",
    "    'playground': \"Playground\",\n",
    "    'stable_diffusion_3.5_medium': \"SD3.5M\",\n",
    "    'sd3_medium_grpo': \"SD3.5M-PickScore\",\n",
    "    'sd3_medium_grpo_geneval': \"SD3.5M-GenEval\",\n",
    "    'nano-banana': \"Nano Banana\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ead43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "relative_idx_list = [10, 170, 78, 48]\n",
    "\n",
    "for relative_idx in relative_idx_list:\n",
    "    slice = np.where(np.array(dataset_no_image[\"index\"]) == relative_idx)[0]\n",
    "    df = dataset[slice]\n",
    "    n = len(df[\"model\"]) - 1\n",
    "    text_str = \", \".join([s.title() for s in json.loads(df[\"selected_dims\"][0])])\n",
    "    # text_str = df[\"distorted_prompt\"][0]\n",
    "\n",
    "    pylab.figure(figsize=(28, 4))\n",
    "    for i in range(n+1):\n",
    "        if df[\"model\"][i] not in orders:\n",
    "            continue\n",
    "        ax = pylab.subplot(1, n, orders.index(df[\"model\"][i]) + 1)\n",
    "        pylab.imshow(dataset[\"image_distorted\"][slice[i]])\n",
    "        pylab.axis(\"off\")\n",
    "        ax.set_title(name_mapping[df[\"model\"][i]], fontsize=18, pad=10)\n",
    "\n",
    "    pylab.suptitle(text_str, y=0.02, fontsize=24, ha=\"center\", va=\"bottom\")\n",
    "    pylab.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    pylab.savefig(\"qualitative_\" + str(relative_idx) + \".pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c09c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d62a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f55d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_failed_images(sample):\n",
    "    original_image = np.array(sample[\"image_original\"])\n",
    "    distorted_image = np.array(sample[\"image_distorted\"])\n",
    "    if original_image.mean((0,1))[2]>250:\n",
    "        # the whole image is with color blue\n",
    "        return True\n",
    "    if distorted_image.mean((0,1))[2]>250:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8fcf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = [] \n",
    "for i in range(len(ds)): \n",
    "    sample = ds[i]\n",
    "    returns.append(find_failed_images(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c35ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(returns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "image_client = OpenAI() \n",
    "openrouter_api_key = os.environ[\"OPENROUTER_API_KEY\"]\n",
    "openrouter_client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=openrouter_api_key)\n",
    "llm_client = openrouter_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa47b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def nano_banana_generate(prompt: str) -> Image.Image:\n",
    "    last_error = None\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            completion = openrouter_client.chat.completions.create(\n",
    "                model=\"google/gemini-2.5-flash-image-preview\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": f\"Generate an image with following prompt, return the image directly: {prompt}\",\n",
    "                            },\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            message = completion.choices[0].message\n",
    "            try:\n",
    "                image_url = message.images[0][\"image_url\"][\"url\"]\n",
    "                image_bytes = base64.b64decode(image_url.split(\",\")[1].replace(\"\\x00\", \"\"))\n",
    "                return Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            except AttributeError:\n",
    "                text_response = getattr(message, \"content\", \"\")\n",
    "                print(f\"nano-banana returned text-only response: {text_response}\")\n",
    "                raise Exception(\"nano-banana returned text-only response\")\n",
    "        except Exception as exc:\n",
    "            last_error = exc\n",
    "            if attempt == 4:\n",
    "                print(f\"nano-banana failed after retries: {exc}\")\n",
    "                return Image.new(\"RGB\", (1024, 1024), color=(0, 0, 255))\n",
    "            print(f\"retrying nano-banana due to error: {exc}\")\n",
    "    return Image.new(\"RGB\", (1024, 1024), color=(0, 0, 255))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67af9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.select(np.where(returns)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "cc8ae6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c78a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = np.where(returns)[0].tolist()\n",
    "for row_id, sample in zip(idx, dataset.select(idx)):\n",
    "    prompt_original = sample[\"prompt_original\"]\n",
    "    prompt_distorted = sample[\"prompt_distorted\"]\n",
    "    print(prompt_original)\n",
    "    img = nano_banana_generate(prompt_original)\n",
    "    img_2 = nano_banana_generate(prompt_distorted)\n",
    "    dataset[row_id][\"image_original\"] = img\n",
    "    dataset[row_id][\"image_distorted\"] = img_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "70e9e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[row_id][\"image_original\"] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(returns)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "fingerprint = hashlib.md5(b\"add_images_v1\").hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "967e81e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function add_images at 0x7b75ddfdd240> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785d345119c149c6a8d55325b19d9e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrying nano-banana due to error: 'ChatCompletionMessage' object has no attribute 'images'\n",
      "retrying nano-banana due to error: 'ChatCompletionMessage' object has no attribute 'images'\n",
      "retrying nano-banana due to error: 'ChatCompletionMessage' object has no attribute 'images'\n",
      "retrying nano-banana due to error: 'ChatCompletionMessage' object has no attribute 'images'\n",
      "retrying nano-banana due to error: 'ChatCompletionMessage' object has no attribute 'images'\n"
     ]
    }
   ],
   "source": [
    "idx = np.where(returns)[0].tolist()\n",
    "\n",
    "def add_images(example, i):\n",
    "    if i in idx:\n",
    "        example[\"image_original\"] = nano_banana_generate(example[\"prompt_original\"])\n",
    "        example[\"image_distorted\"] = nano_banana_generate(example[\"prompt_distorted\"])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_images, with_indices=True, num_proc=8, new_fingerprint=fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "6d43ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = [] \n",
    "for i in range(300):\n",
    "    sample = ds[i]\n",
    "    returns.append(find_failed_images(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "ea25e806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  8,  28,  45,  54,  77,  84,  91,  92, 102, 121, 124, 125, 143,\n",
       "        189, 195, 210, 221, 234, 237, 244, 245, 280, 285, 292]),)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(returns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"weathon/aas_benchmark_2\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171832d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_original', 'image_distorted', 'index', 'prompt_original', 'prompt_distorted', 'selected_dims', 'hpsv2_reward', 'hpsv3_reward', 'image_reward', 'rater', 'llm_selected', 'blip_score', 'model'],\n",
       "    num_rows: 3300\n",
       "})"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e81160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.remove_columns(['llm_judge', 'blip_selected']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'hpsv3_reward', 'rater'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17399da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_interest = [  8,  28,  45,  54,  77,  84,  91,  92, 102, 121, 124, 125, 143, 189, 195, 210, 221, 234, 237, 244, 245, 280, 285, 292]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b31c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bb5dc3cc3d49abb024032e1d29faf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d892189b50142b9b392bec466feff88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00015.parquet:   0%|          | 0.00/731M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4fae59af4446fbaa63e0d2544e2202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00015.parquet:   0%|          | 0.00/685M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7768d78c71435999366665ea817b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"weathon/aas_benchmark_2\", split=\"train\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333000a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image \n",
    "from transformers import BlipProcessor, BlipForImageTextRetrieval\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-large-coco\")\n",
    "model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-large-coco\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "def blip_select(sample, i):\n",
    "    if i not in idx_of_interest:\n",
    "        return sample[\"blip_score\"]\n",
    "    print(\"Processing index:\", i)\n",
    "    inputs = processor([sample[\"image_original\"], sample[\"image_distorted\"]], [sample[\"prompt_distorted\"]]*2, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "\n",
    "    itm_scores = model(**inputs)[0]\n",
    "    # sample[\"blip_select\"] = itm_scores[:,1].argmax()\n",
    "    return itm_scores.cpu().detach().numpy().tolist()\n",
    "    \n",
    "import tqdm\n",
    "blip_score = []\n",
    "for i in tqdm.tqdm(range(len(dataset))):\n",
    "    blip_score.append(blip_select(dataset[i], i))\n",
    "dataset = dataset.remove_columns(\"blip_score\").add_column(\"blip_score\", blip_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "f43f0daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9230052b045046c2ab9a1e6c67992929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/15 shards):   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"aas_benchmark_2_with_blip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3acfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2ed4a9e2d840b5bcf78e3f2cf01bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472ecdfa53b4492aa6524db64e4a3ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import infer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d9147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"aas_benchmark_2_with_blip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00796bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_interest = [  8,  28,  45,  54,  77,  84,  91,  92, 102, 121, 124, 125, 143, 189, 195, 210, 221, 234, 237, 244, 245, 280, 285, 292]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cfa741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/3300 [00:00<03:43, 14.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.90625 original\n",
      "clarity 1.046875 original\n",
      "color aesthetic 0.90625 original\n",
      "color brightness 0.7421875 original\n",
      "detail realism 0.77734375 original\n",
      "detail refinement 1.109375 original\n",
      "emotion 1.921875 original\n",
      "lighting distinction 0.2578125 original\n",
      "main object 2.0 original\n",
      "object pairing 0.255859375 original\n",
      "richness 1.4921875 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/3300 [00:05<33:31,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.02001953125 distorted\n",
      "clarity 0.9453125 distorted\n",
      "color aesthetic 0.470703125 distorted\n",
      "color brightness 0.65625 distorted\n",
      "detail realism 0.70703125 distorted\n",
      "detail refinement 0.5 distorted\n",
      "emotion 0.8359375 distorted\n",
      "lighting distinction 0.8671875 distorted\n",
      "main object 1.1015625 distorted\n",
      "object pairing 0.2236328125 distorted\n",
      "richness 1.234375 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/3300 [00:06<04:41, 11.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.018798828125 original\n",
      "clarity 1.984375 original\n",
      "color aesthetic 0.87109375 original\n",
      "color brightness 0.040771484375 original\n",
      "detail realism 0.53515625 original\n",
      "detail refinement 1.5234375 original\n",
      "emotion 1.9921875 original\n",
      "lighting distinction 0.3125 original\n",
      "main object 2.0 original\n",
      "object pairing 0.6328125 original\n",
      "richness 1.5 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 31/3300 [00:10<24:47,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.0927734375 distorted\n",
      "clarity 1.375 distorted\n",
      "color aesthetic 0.89453125 distorted\n",
      "color brightness 0.0068359375 distorted\n",
      "detail realism 0.87109375 distorted\n",
      "detail refinement 1.125 distorted\n",
      "emotion 0.91015625 distorted\n",
      "lighting distinction 0.1142578125 distorted\n",
      "main object 1.9609375 distorted\n",
      "object pairing 0.75390625 distorted\n",
      "richness 1.0234375 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 45/3300 [00:10<05:00, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.0830078125 original\n",
      "clarity 1.9453125 original\n",
      "color aesthetic 0.97265625 original\n",
      "color brightness 0.004425048828125 original\n",
      "detail realism 0.6640625 original\n",
      "detail refinement 1.28125 original\n",
      "emotion 1.28125 original\n",
      "lighting distinction 0.67578125 original\n",
      "main object 2.0 original\n",
      "object pairing 0.8828125 original\n",
      "richness 1.8359375 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 49/3300 [00:12<14:34,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.65625 distorted\n",
      "clarity 1.2890625 distorted\n",
      "color aesthetic 0.65234375 distorted\n",
      "color brightness 0.6796875 distorted\n",
      "detail realism 0.0859375 distorted\n",
      "detail refinement 0.421875 distorted\n",
      "emotion 0.59765625 distorted\n",
      "lighting distinction 1.03125 distorted\n",
      "main object 1.796875 distorted\n",
      "object pairing 0.5 distorted\n",
      "richness 1.484375 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 53/3300 [00:13<08:53,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.44921875 original\n",
      "clarity 1.796875 original\n",
      "color aesthetic 0.6875 original\n",
      "color brightness 0.8125 original\n",
      "detail realism 0.2080078125 original\n",
      "detail refinement 1.203125 original\n",
      "emotion 1.0234375 original\n",
      "lighting distinction 1.734375 original\n",
      "main object 1.9921875 original\n",
      "object pairing 0.70703125 original\n",
      "richness 1.4375 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 57/3300 [00:15<16:29,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.7578125 distorted\n",
      "clarity 1.0078125 distorted\n",
      "color aesthetic 0.8671875 distorted\n",
      "color brightness 0.05810546875 distorted\n",
      "detail realism 0.80859375 distorted\n",
      "detail refinement 0.85546875 distorted\n",
      "emotion 0.8359375 distorted\n",
      "lighting distinction 0.416015625 distorted\n",
      "main object 1.9921875 distorted\n",
      "object pairing 0.02783203125 distorted\n",
      "richness 1.3359375 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 77/3300 [00:16<03:37, 14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.076171875 original\n",
      "clarity 1.6015625 original\n",
      "color aesthetic 1.6875 original\n",
      "color brightness 0.0634765625 original\n",
      "detail realism 0.1494140625 original\n",
      "detail refinement 1.5546875 original\n",
      "emotion 1.46875 original\n",
      "lighting distinction 0.6796875 original\n",
      "main object 2.0 original\n",
      "object pairing 0.0673828125 original\n",
      "richness 1.7109375 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 79/3300 [00:19<27:36,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.0096435546875 distorted\n",
      "clarity 0.921875 distorted\n",
      "color aesthetic 0.73046875 distorted\n",
      "color brightness 0.10693359375 distorted\n",
      "detail realism 0.2021484375 distorted\n",
      "detail refinement 0.5546875 distorted\n",
      "emotion 0.86328125 distorted\n",
      "lighting distinction 1.0625 distorted\n",
      "main object 1.9921875 distorted\n",
      "object pairing 0.69921875 distorted\n",
      "richness 1.0390625 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 83/3300 [00:19<15:09,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.0289306640625 original\n",
      "clarity 1.0 original\n",
      "color aesthetic 1.9140625 original\n",
      "color brightness 0.1708984375 original\n",
      "detail realism 0.10791015625 original\n",
      "detail refinement 1.25 original\n",
      "emotion 1.921875 original\n",
      "lighting distinction 1.59375 original\n",
      "main object 1.9921875 original\n",
      "object pairing 0.181640625 original\n",
      "richness 1.2890625 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 87/3300 [00:23<29:47,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.53125 distorted\n",
      "clarity 0.7890625 distorted\n",
      "color aesthetic 0.22265625 distorted\n",
      "color brightness 0.00469970703125 distorted\n",
      "detail realism 0.1845703125 distorted\n",
      "detail refinement 0.609375 distorted\n",
      "emotion 0.56640625 distorted\n",
      "lighting distinction 0.013671875 distorted\n",
      "main object 1.7890625 distorted\n",
      "object pairing 0.77734375 distorted\n",
      "richness 0.90625 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 91/3300 [00:24<16:15,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.2060546875 original\n",
      "clarity 1.859375 original\n",
      "color aesthetic 0.9921875 original\n",
      "color brightness 0.8984375 original\n",
      "detail realism 0.828125 original\n",
      "detail refinement 1.2421875 original\n",
      "emotion 0.53125 original\n",
      "lighting distinction 0.6171875 original\n",
      "main object 1.421875 original\n",
      "object pairing 0.1630859375 original\n",
      "richness 1.8359375 original\n",
      "background 0.984375 distorted\n",
      "clarity 0.9609375 distorted\n",
      "color aesthetic 0.26953125 distorted\n",
      "color brightness 0.78515625 distorted\n",
      "detail realism 0.875 distorted\n",
      "detail refinement 0.41015625 distorted\n",
      "emotion 0.671875 distorted\n",
      "lighting distinction 0.2353515625 distorted\n",
      "main object 0.7109375 distorted\n",
      "object pairing 0.70703125 distorted\n",
      "richness 1.34375 distorted\n",
      "background 0.068359375 original\n",
      "clarity 1.984375 original\n",
      "color aesthetic 1.7578125 original\n",
      "color brightness 0.4453125 original\n",
      "detail realism 0.439453125 original\n",
      "detail refinement 1.6484375 original\n",
      "emotion 1.9609375 original\n",
      "lighting distinction 0.5390625 original\n",
      "main object 2.0 original\n",
      "object pairing 0.8984375 original\n",
      "richness 1.171875 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 95/3300 [00:31<52:18,  1.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.89453125 distorted\n",
      "clarity 1.9375 distorted\n",
      "color aesthetic 0.7578125 distorted\n",
      "color brightness 0.6328125 distorted\n",
      "detail realism 0.0233154296875 distorted\n",
      "detail refinement 0.82421875 distorted\n",
      "emotion 0.9921875 distorted\n",
      "lighting distinction 0.90625 distorted\n",
      "main object 2.0 distorted\n",
      "object pairing 0.89453125 distorted\n",
      "richness 1.0546875 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 101/3300 [00:32<20:04,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.1171875 original\n",
      "clarity 1.9765625 original\n",
      "color aesthetic 0.91796875 original\n",
      "color brightness 0.4296875 original\n",
      "detail realism 0.44921875 original\n",
      "detail refinement 1.3046875 original\n",
      "emotion 0.66796875 original\n",
      "lighting distinction 0.98828125 original\n",
      "main object 1.9921875 original\n",
      "object pairing 0.11962890625 original\n",
      "richness 1.4921875 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 105/3300 [00:35<27:23,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.59375 distorted\n",
      "clarity 0.43359375 distorted\n",
      "color aesthetic 0.2451171875 distorted\n",
      "color brightness 0.921875 distorted\n",
      "detail realism 0.10791015625 distorted\n",
      "detail refinement 0.1572265625 distorted\n",
      "emotion 0.69140625 distorted\n",
      "lighting distinction 0.98828125 distorted\n",
      "main object 0.9453125 distorted\n",
      "object pairing 0.408203125 distorted\n",
      "richness 1.234375 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 121/3300 [00:36<04:32, 11.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.47265625 original\n",
      "clarity 1.1328125 original\n",
      "color aesthetic 0.9609375 original\n",
      "color brightness 0.00994873046875 original\n",
      "detail realism 0.921875 original\n",
      "detail refinement 0.7578125 original\n",
      "emotion 1.7265625 original\n",
      "lighting distinction 0.0927734375 original\n",
      "main object 1.96875 original\n",
      "object pairing 0.1953125 original\n",
      "richness 1.8515625 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 123/3300 [00:38<18:59,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.228515625 distorted\n",
      "clarity 1.046875 distorted\n",
      "color aesthetic 0.8671875 distorted\n",
      "color brightness 0.2080078125 distorted\n",
      "detail realism 0.890625 distorted\n",
      "detail refinement 0.7890625 distorted\n",
      "emotion 0.61328125 distorted\n",
      "lighting distinction 0.59375 distorted\n",
      "main object 0.734375 distorted\n",
      "object pairing 0.0712890625 distorted\n",
      "richness 1.109375 distorted\n",
      "background 0.0927734375 original\n",
      "clarity 1.9609375 original\n",
      "color aesthetic 0.9765625 original\n",
      "color brightness 0.0272216796875 original\n",
      "detail realism 0.828125 original\n",
      "detail refinement 1.4296875 original\n",
      "emotion 1.8125 original\n",
      "lighting distinction 0.341796875 original\n",
      "main object 1.9921875 original\n",
      "object pairing 0.70703125 original\n",
      "richness 1.734375 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 125/3300 [00:42<45:23,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.48046875 distorted\n",
      "clarity 0.8828125 distorted\n",
      "color aesthetic 0.73046875 distorted\n",
      "color brightness 0.31640625 distorted\n",
      "detail realism 0.796875 distorted\n",
      "detail refinement 0.80859375 distorted\n",
      "emotion 0.80078125 distorted\n",
      "lighting distinction 0.2734375 distorted\n",
      "main object 1.765625 distorted\n",
      "object pairing 0.34765625 distorted\n",
      "richness 1.40625 distorted\n",
      "background 0.033203125 original\n",
      "clarity 1.9375 original\n",
      "color aesthetic 1.921875 original\n",
      "color brightness 0.0673828125 original\n",
      "detail realism 0.5703125 original\n",
      "detail refinement 1.3046875 original\n",
      "emotion 1.75 original\n",
      "lighting distinction 0.2255859375 original\n",
      "main object 2.0 original\n",
      "object pairing 0.34375 original\n",
      "richness 1.8359375 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 128/3300 [00:46<49:15,  1.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.0849609375 distorted\n",
      "clarity 0.1103515625 distorted\n",
      "color aesthetic 0.46875 distorted\n",
      "color brightness 0.3203125 distorted\n",
      "detail realism 0.578125 distorted\n",
      "detail refinement 0.1484375 distorted\n",
      "emotion 0.6640625 distorted\n",
      "lighting distinction 0.81640625 distorted\n",
      "main object 0.1650390625 distorted\n",
      "object pairing 0.73046875 distorted\n",
      "richness 1.25 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 142/3300 [00:47<06:29,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.115234375 original\n",
      "clarity 1.9453125 original\n",
      "color aesthetic 1.859375 original\n",
      "color brightness 0.130859375 original\n",
      "detail realism 0.5703125 original\n",
      "detail refinement 1.390625 original\n",
      "emotion 1.890625 original\n",
      "lighting distinction 0.5859375 original\n",
      "main object 1.9765625 original\n",
      "object pairing 0.1162109375 original\n",
      "richness 1.6171875 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 146/3300 [00:51<25:28,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.07373046875 distorted\n",
      "clarity 1.4921875 distorted\n",
      "color aesthetic 0.6796875 distorted\n",
      "color brightness 0.275390625 distorted\n",
      "detail realism 0.44921875 distorted\n",
      "detail refinement 0.96875 distorted\n",
      "emotion 0.4765625 distorted\n",
      "lighting distinction 0.1572265625 distorted\n",
      "main object 1.9765625 distorted\n",
      "object pairing 0.73046875 distorted\n",
      "richness 1.171875 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 188/3300 [00:53<03:06, 16.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.05322265625 original\n",
      "clarity 1.71875 original\n",
      "color aesthetic 0.92578125 original\n",
      "color brightness 0.0028533935546875 original\n",
      "detail realism 0.51171875 original\n",
      "detail refinement 1.3671875 original\n",
      "emotion 0.9609375 original\n",
      "lighting distinction 0.21875 original\n",
      "main object 1.9375 original\n",
      "object pairing 0.1171875 original\n",
      "richness 1.5 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 192/3300 [00:57<24:11,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.09716796875 distorted\n",
      "clarity 0.66015625 distorted\n",
      "color aesthetic 0.62109375 distorted\n",
      "color brightness 0.004730224609375 distorted\n",
      "detail realism 0.2265625 distorted\n",
      "detail refinement 0.50390625 distorted\n",
      "emotion 0.9375 distorted\n",
      "lighting distinction 0.3515625 distorted\n",
      "main object 1.9921875 distorted\n",
      "object pairing 0.4375 distorted\n",
      "richness 1.4921875 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 194/3300 [00:57<17:46,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.16796875 original\n",
      "clarity 1.8984375 original\n",
      "color aesthetic 1.71875 original\n",
      "color brightness 0.00061798095703125 original\n",
      "detail realism 0.55859375 original\n",
      "detail refinement 1.40625 original\n",
      "emotion 1.5546875 original\n",
      "lighting distinction 0.181640625 original\n",
      "main object 1.9921875 original\n",
      "object pairing 0.75390625 original\n",
      "richness 1.3125 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 198/3300 [01:01<32:06,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.294921875 distorted\n",
      "clarity 0.46875 distorted\n",
      "color aesthetic 0.3203125 distorted\n",
      "color brightness 1.5078125 distorted\n",
      "detail realism 0.1845703125 distorted\n",
      "detail refinement 0.3359375 distorted\n",
      "emotion 0.71875 distorted\n",
      "lighting distinction 0.076171875 distorted\n",
      "main object 1.765625 distorted\n",
      "object pairing 0.248046875 distorted\n",
      "richness 0.703125 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 210/3300 [01:02<06:24,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.0250244140625 original\n",
      "clarity 1.609375 original\n",
      "color aesthetic 0.82421875 original\n",
      "color brightness 1.578125 original\n",
      "detail realism 0.546875 original\n",
      "detail refinement 1.03125 original\n",
      "emotion 1.765625 original\n",
      "lighting distinction 0.984375 original\n",
      "main object 1.9921875 original\n",
      "object pairing 0.23046875 original\n",
      "richness 1.4921875 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 212/3300 [01:06<36:38,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.044677734375 distorted\n",
      "clarity 0.890625 distorted\n",
      "color aesthetic 0.89453125 distorted\n",
      "color brightness 0.0020904541015625 distorted\n",
      "detail realism 0.68359375 distorted\n",
      "detail refinement 0.57421875 distorted\n",
      "emotion 0.25390625 distorted\n",
      "lighting distinction 0.298828125 distorted\n",
      "main object 2.0 distorted\n",
      "object pairing 0.4375 distorted\n",
      "richness 1.2109375 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 220/3300 [01:07<11:07,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.2216796875 original\n",
      "clarity 1.6171875 original\n",
      "color aesthetic 1.3984375 original\n",
      "color brightness 0.0027313232421875 original\n",
      "detail realism 0.33203125 original\n",
      "detail refinement 1.2421875 original\n",
      "emotion 1.7421875 original\n",
      "lighting distinction 0.47265625 original\n",
      "main object 2.0 original\n",
      "object pairing 0.07958984375 original\n",
      "richness 1.59375 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 224/3300 [01:11<28:30,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.0159912109375 distorted\n",
      "clarity 0.8984375 distorted\n",
      "color aesthetic 0.53125 distorted\n",
      "color brightness 0.3515625 distorted\n",
      "detail realism 0.0966796875 distorted\n",
      "detail refinement 0.296875 distorted\n",
      "emotion 0.8359375 distorted\n",
      "lighting distinction 0.9296875 distorted\n",
      "main object 1.296875 distorted\n",
      "object pairing 0.349609375 distorted\n",
      "richness 1.28125 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 234/3300 [01:12<07:22,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.06005859375 original\n",
      "clarity 1.625 original\n",
      "color aesthetic 1.3984375 original\n",
      "color brightness 0.76171875 original\n",
      "detail realism 0.7578125 original\n",
      "detail refinement 1.3671875 original\n",
      "emotion 0.8984375 original\n",
      "lighting distinction 0.828125 original\n",
      "main object 1.9140625 original\n",
      "object pairing 0.146484375 original\n",
      "richness 1.8515625 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 236/3300 [01:14<20:18,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.984375 distorted\n",
      "clarity 0.8203125 distorted\n",
      "color aesthetic 0.68359375 distorted\n",
      "color brightness 0.69140625 distorted\n",
      "detail realism 0.76171875 distorted\n",
      "detail refinement 0.447265625 distorted\n",
      "emotion 0.8203125 distorted\n",
      "lighting distinction 0.84375 distorted\n",
      "main object 1.7578125 distorted\n",
      "object pairing 0.294921875 distorted\n",
      "richness 1.4296875 distorted\n",
      "background 0.2890625 original\n",
      "clarity 1.8828125 original\n",
      "color aesthetic 1.515625 original\n",
      "color brightness 0.333984375 original\n",
      "detail realism 0.8828125 original\n",
      "detail refinement 1.46875 original\n",
      "emotion 1.75 original\n",
      "lighting distinction 0.330078125 original\n",
      "main object 2.0 original\n",
      "object pairing 0.041748046875 original\n",
      "richness 1.5859375 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 240/3300 [01:17<28:52,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.77734375 distorted\n",
      "clarity 1.28125 distorted\n",
      "color aesthetic 0.73046875 distorted\n",
      "color brightness 0.1201171875 distorted\n",
      "detail realism 0.478515625 distorted\n",
      "detail refinement 0.326171875 distorted\n",
      "emotion 1.0390625 distorted\n",
      "lighting distinction 0.0751953125 distorted\n",
      "main object 1.15625 distorted\n",
      "object pairing 0.458984375 distorted\n",
      "richness 1.0234375 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 244/3300 [01:17<15:41,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.25 original\n",
      "clarity 1.90625 original\n",
      "color aesthetic 1.71875 original\n",
      "color brightness 0.7890625 original\n",
      "detail realism 0.10888671875 original\n",
      "detail refinement 1.234375 original\n",
      "emotion 1.9921875 original\n",
      "lighting distinction 1.15625 original\n",
      "main object 2.0 original\n",
      "object pairing 0.8046875 original\n",
      "richness 1.40625 original\n",
      "background 0.53125 distorted\n",
      "clarity 0.859375 distorted\n",
      "color aesthetic 0.81640625 distorted\n",
      "color brightness 0.6796875 distorted\n",
      "detail realism 0.029541015625 distorted\n",
      "detail refinement 0.310546875 distorted\n",
      "emotion 0.74609375 distorted\n",
      "lighting distinction 0.50390625 distorted\n",
      "main object 0.953125 distorted\n",
      "object pairing 0.470703125 distorted\n",
      "richness 0.59375 distorted\n",
      "background 0.1435546875 original\n",
      "clarity 1.5625 original\n",
      "color aesthetic 0.8984375 original\n",
      "color brightness 0.0908203125 original\n",
      "detail realism 0.3359375 original\n",
      "detail refinement 1.3046875 original\n",
      "emotion 1.4296875 original\n",
      "lighting distinction 0.189453125 original\n",
      "main object 2.0 original\n",
      "object pairing 0.34765625 original\n",
      "richness 1.703125 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 248/3300 [01:25<46:57,  1.08it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.27734375 distorted\n",
      "clarity 0.8203125 distorted\n",
      "color aesthetic 0.1484375 distorted\n",
      "color brightness 0.56640625 distorted\n",
      "detail realism 0.87109375 distorted\n",
      "detail refinement 0.5546875 distorted\n",
      "emotion 0.78515625 distorted\n",
      "lighting distinction 0.05322265625 distorted\n",
      "main object 1.9453125 distorted\n",
      "object pairing 0.18359375 distorted\n",
      "richness 1.0078125 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 280/3300 [01:27<02:58, 16.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.1044921875 original\n",
      "clarity 1.8671875 original\n",
      "color aesthetic 1.90625 original\n",
      "color brightness 0.314453125 original\n",
      "detail realism 0.408203125 original\n",
      "detail refinement 1.40625 original\n",
      "emotion 1.5546875 original\n",
      "lighting distinction 0.236328125 original\n",
      "main object 2.0 original\n",
      "object pairing 0.72265625 original\n",
      "richness 1.3125 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 282/3300 [01:31<33:51,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.0791015625 distorted\n",
      "clarity 1.921875 distorted\n",
      "color aesthetic 0.70703125 distorted\n",
      "color brightness 0.008544921875 distorted\n",
      "detail realism 0.59765625 distorted\n",
      "detail refinement 1.28125 distorted\n",
      "emotion 0.78515625 distorted\n",
      "lighting distinction 1.3046875 distorted\n",
      "main object 2.0 distorted\n",
      "object pairing 0.53125 distorted\n",
      "richness 1.125 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 284/3300 [01:31<24:34,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.47265625 original\n",
      "clarity 1.984375 original\n",
      "color aesthetic 1.9453125 original\n",
      "color brightness 0.41015625 original\n",
      "detail realism 0.70703125 original\n",
      "detail refinement 1.6015625 original\n",
      "emotion 1.2578125 original\n",
      "lighting distinction 0.84765625 original\n",
      "main object 2.0 original\n",
      "object pairing 0.8671875 original\n",
      "richness 1.34375 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 288/3300 [01:33<23:19,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.0224609375 distorted\n",
      "clarity 1.8359375 distorted\n",
      "color aesthetic 0.8984375 distorted\n",
      "color brightness 0.0703125 distorted\n",
      "detail realism 0.84375 distorted\n",
      "detail refinement 1.25 distorted\n",
      "emotion 0.8203125 distorted\n",
      "lighting distinction 0.10400390625 distorted\n",
      "main object 2.0 distorted\n",
      "object pairing 0.271484375 distorted\n",
      "richness 1.2890625 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 292/3300 [01:33<12:58,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.00860595703125 original\n",
      "clarity 1.953125 original\n",
      "color aesthetic 1.03125 original\n",
      "color brightness 0.8515625 original\n",
      "detail realism 0.546875 original\n",
      "detail refinement 1.328125 original\n",
      "emotion 1.9296875 original\n",
      "lighting distinction 0.4609375 original\n",
      "main object 2.0 original\n",
      "object pairing 0.1962890625 original\n",
      "richness 1.5 original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 296/3300 [01:35<17:38,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background 0.80078125 distorted\n",
      "clarity 1.875 distorted\n",
      "color aesthetic 0.7578125 distorted\n",
      "color brightness 0.6796875 distorted\n",
      "detail realism 0.88671875 distorted\n",
      "detail refinement 1.03125 distorted\n",
      "emotion 0.9453125 distorted\n",
      "lighting distinction 0.515625 distorted\n",
      "main object 2.0 distorted\n",
      "object pairing 0.051513671875 distorted\n",
      "richness 1.234375 distorted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3300/3300 [03:37<00:00, 15.17it/s]\n"
     ]
    }
   ],
   "source": [
    "rater_results = [] \n",
    "import tqdm \n",
    "for i, sample in enumerate(tqdm.tqdm(dataset)):\n",
    "    result = infer.rate_image(sample, i, set(idx_of_interest))\n",
    "    rater_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rater both score different than last time, i got it, aaaaaa it is so simple, boh images are regenerated!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e967e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(rater_results) == len(dataset[\"rater\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f64e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(\"rater\").add_column(\"rater\", rater_results)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca722e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22800ef32f942a6aa39257e599f67bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/15 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c170d64ab934eee83212432e845b48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d641fb3b0f1e488f831978246d7278c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7805eb923d0945869ccc44a7ba856484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9f5772c4924c23994f16768bb3a95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"weathon/aas_benchmark_2\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "35f6ad4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c687595d137f4c4b970fe84400552420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/15 shards):   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"aas_benchmark_2_with_blip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b638aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i, sample in enumerate(tqdm.tqdm(dataset)):\n",
    "    if i not in idx_of_interest:\n",
    "        continue\n",
    "    else: \n",
    "        print(i)\n",
    "        time.sleep(1)\n",
    "        print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "2a5f76d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_original', 'image_distorted', 'index', 'prompt_original', 'prompt_distorted', 'selected_dims', 'llm_judge', 'hpsv2_reward', 'hpsv3_reward', 'image_reward', 'rater', 'llm_selected', 'blip_selected', 'model', 'blip_score'],\n",
       "    num_rows: 3300\n",
       "})"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c36ec99",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DistributedTensorGatherer' from 'transformers.trainer' (/home/wg25r/miniconda/envs/neg/lib/python3.10/site-packages/transformers/trainer.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[425], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhpsv3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HPSv3RewardInferencer\n\u001b[1;32m      3\u001b[0m inferencer \u001b[38;5;241m=\u001b[39m HPSv3RewardInferencer(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/neg/lib/python3.10/site-packages/hpsv3/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HPSv3RewardInferencer\n",
      "File \u001b[0;32m~/miniconda/envs/neg/lib/python3.10/site-packages/hpsv3/inference.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator_qwen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prompt_with_special_token, prompt_without_special_token, INSTRUCTION\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelConfig, PEFTLoraConfig, TrainingConfig, DataConfig, parse_args_with_yaml\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model_and_processor\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     12\u001b[0m _MODEL_CONFIG_PATH \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/neg/lib/python3.10/site-packages/hpsv3/train.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhpsv3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqwen2vl_trainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Qwen2VLRewardModelBT,\n\u001b[1;32m     10\u001b[0m     VLMRewardTrainer,\n\u001b[1;32m     11\u001b[0m     compute_multi_attr_accuracy,\n\u001b[1;32m     12\u001b[0m     PartialEmbeddingUpdateCallback,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhpsv3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PairwiseOriginalDataset\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhpsv3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator_qwen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QWen2VLDataCollator\n",
      "File \u001b[0;32m~/miniconda/envs/neg/lib/python3.10/site-packages/hpsv3/model/qwen2vl_trainer.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainerCallback\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     is_sagemaker_mp_enabled,\n\u001b[1;32m     28\u001b[0m     is_peft_available,\n\u001b[1;32m     29\u001b[0m     is_datasets_available,\n\u001b[1;32m     30\u001b[0m     WEIGHTS_NAME,\n\u001b[1;32m     31\u001b[0m     TRAINING_ARGS_NAME,\n\u001b[1;32m     32\u001b[0m     SAFE_WEIGHTS_NAME,\n\u001b[1;32m     33\u001b[0m     TRAINER_STATE_NAME,\n\u001b[1;32m     34\u001b[0m     PREFIX_CHECKPOINT_DIR,\n\u001b[1;32m     35\u001b[0m     logger,\n\u001b[1;32m     36\u001b[0m     speed_metrics,\n\u001b[1;32m     37\u001b[0m     deepspeed_init,\n\u001b[1;32m     38\u001b[0m     speed_metrics,\n\u001b[1;32m     39\u001b[0m     has_length,\n\u001b[1;32m     40\u001b[0m     EvalPrediction,\n\u001b[1;32m     41\u001b[0m     EvalLoopContainer,\n\u001b[1;32m     42\u001b[0m     PredictionOutput,\n\u001b[1;32m     43\u001b[0m     is_torch_xla_available,\n\u001b[1;32m     44\u001b[0m     denumpify_detensorize,\n\u001b[1;32m     45\u001b[0m     PredictionOutput,\n\u001b[1;32m     46\u001b[0m     EvalLoopOutput,\n\u001b[1;32m     47\u001b[0m     DistributedTensorGatherer,\n\u001b[1;32m     48\u001b[0m     SequentialDistributedSampler,\n\u001b[1;32m     49\u001b[0m     nested_concat,\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_pt_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IterableDatasetShard\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_callback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainerControl, TrainerState\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DistributedTensorGatherer' from 'transformers.trainer' (/home/wg25r/miniconda/envs/neg/lib/python3.10/site-packages/transformers/trainer.py)"
     ]
    }
   ],
   "source": [
    "from hpsv3 import HPSv3RewardInferencer \n",
    "\n",
    "inferencer = HPSv3RewardInferencer(device='cuda:2')\n",
    "\n",
    "import torch\n",
    "def hpsv3_reward(sample, i): \n",
    "    if i not in idx_of_interest:\n",
    "        return sample[\"hpsv3_reward\"]\n",
    "    images_part = [sample[\"image_original\"], sample[\"image_original\"], sample[\"image_distorted\"],  sample[\"image_distorted\"]]\n",
    "    prompts_part = [\n",
    "        sample[\"prompt_original\"],\n",
    "        sample[\"prompt_distorted\"],\n",
    "        sample[\"prompt_original\"],\n",
    "        sample[\"prompt_distorted\"]\n",
    "    ] \n",
    "    with torch.no_grad(): \n",
    "        with torch.cuda.amp.autocast():\n",
    "            rewards = inferencer.reward(prompts=prompts_part, image_paths=images_part)\n",
    "    results = {\n",
    "        \"hpsv3_oiop\": rewards[0], # original image, original prompt\n",
    "        \"hpsv3_oidp\": rewards[1], # original image, distorted prompt\n",
    "        \"hpsv3_diop\": rewards[2], # distorted image, original prompt\n",
    "        \"hpsv3_didp\": rewards[3], # distorted image, distorted prompt \n",
    "    }\n",
    "    return results \n",
    "  \n",
    "rewards = []\n",
    "import tqdm\n",
    "for sample in tqdm.tqdm(dataset):\n",
    "    reward = hpsv3_reward(sample)\n",
    "    rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cb181d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "# dataset = load_dataset(\"weathon/aas_benchmark\", split=\"train\")\n",
    "# dataset = load_from_disk(\"aas_benchmark_2_with_blip_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"../hpsv2_rewards.json\", \"r\") as f:\n",
    "#     hpsv2_rewards = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eef9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_original', 'image_distorted', 'index', 'prompt_original', 'prompt_distorted', 'selected_dims', 'llm_judge', 'hpsv3_reward', 'image_reward', 'rater', 'llm_selected', 'blip_selected', 'model', 'blip_score', 'hpsv2_reward'],\n",
       "    num_rows: 3300\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.remove_columns(\"hpsv2_reward\").add_column(\"hpsv2_reward\", hpsv2_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7e00b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea4c8de2ec04ce597e33ef6f2909d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/15 shards):   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset.save_to_disk(\"aas_benchmark_2_with_blip_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6956fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"aas_benchmark_2_with_blip_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae90d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from /home/wg25r/.cache/ImageReward/ImageReward.pt\n",
      "checkpoint loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/3300 [00:01<06:31,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.6318359375, 'image_reward_oidp': 1.4033203125, 'image_reward_diop': -1.9599609375, 'image_reward_didp': -0.068115234375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 32/3300 [00:02<04:22, 12.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.6728515625, 'image_reward_oidp': 1.626953125, 'image_reward_diop': 0.37451171875, 'image_reward_didp': 0.35888671875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 48/3300 [00:03<04:25, 12.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.8701171875, 'image_reward_oidp': 0.00301361083984375, 'image_reward_diop': 1.908203125, 'image_reward_didp': 1.80078125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 58/3300 [00:04<04:32, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.0, 'image_reward_oidp': 1.470703125, 'image_reward_diop': -0.296142578125, 'image_reward_didp': 1.41796875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 80/3300 [00:06<04:23, 12.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.7587890625, 'image_reward_oidp': 1.572265625, 'image_reward_diop': 1.1142578125, 'image_reward_didp': 1.4326171875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 88/3300 [00:06<04:31, 11.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.3564453125, 'image_reward_oidp': 0.5126953125, 'image_reward_diop': 0.451416015625, 'image_reward_didp': 1.4580078125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 92/3300 [00:07<05:32,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 0.6748046875, 'image_reward_oidp': 1.412109375, 'image_reward_diop': 0.230224609375, 'image_reward_didp': 0.98095703125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 96/3300 [00:07<05:24,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.9345703125, 'image_reward_oidp': 1.90234375, 'image_reward_diop': 1.77734375, 'image_reward_didp': 1.892578125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 106/3300 [00:08<04:37, 11.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.4287109375, 'image_reward_oidp': 1.275390625, 'image_reward_diop': -0.2071533203125, 'image_reward_didp': 0.492919921875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 124/3300 [00:09<04:19, 12.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 0.180908203125, 'image_reward_oidp': -0.5830078125, 'image_reward_diop': -0.488037109375, 'image_reward_didp': 1.2646484375}\n",
      "{'image_reward_oiop': 1.3623046875, 'image_reward_oidp': 0.7373046875, 'image_reward_diop': -0.1895751953125, 'image_reward_didp': 1.1044921875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 128/3300 [00:10<05:54,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.3544921875, 'image_reward_oidp': 1.49609375, 'image_reward_diop': 1.0849609375, 'image_reward_didp': 1.458984375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 146/3300 [00:11<03:59, 13.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 0.483154296875, 'image_reward_oidp': -0.2220458984375, 'image_reward_diop': -0.38818359375, 'image_reward_didp': 0.45947265625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 192/3300 [00:14<04:13, 12.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 0.2802734375, 'image_reward_oidp': -0.152587890625, 'image_reward_diop': -0.6982421875, 'image_reward_didp': 1.2197265625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 198/3300 [00:14<04:27, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.837890625, 'image_reward_oidp': 0.2264404296875, 'image_reward_diop': -0.7275390625, 'image_reward_didp': 0.849609375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 214/3300 [00:16<04:07, 12.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.798828125, 'image_reward_oidp': 1.5849609375, 'image_reward_diop': -0.7998046875, 'image_reward_didp': 0.78515625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 224/3300 [00:16<04:15, 12.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.6865234375, 'image_reward_oidp': 1.59765625, 'image_reward_diop': 0.6123046875, 'image_reward_didp': 1.1005859375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 236/3300 [00:17<04:40, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.162109375, 'image_reward_oidp': 0.875, 'image_reward_diop': 0.144287109375, 'image_reward_didp': 1.34765625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 240/3300 [00:18<04:49, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.2841796875, 'image_reward_oidp': 0.1865234375, 'image_reward_diop': 0.970703125, 'image_reward_didp': 1.02734375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 244/3300 [00:18<03:59, 12.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.26171875, 'image_reward_oidp': 0.260498046875, 'image_reward_diop': 0.1597900390625, 'image_reward_didp': -1.017578125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 248/3300 [00:19<05:33,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 0.62890625, 'image_reward_oidp': -1.2314453125, 'image_reward_diop': 0.529296875, 'image_reward_didp': 1.076171875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 284/3300 [00:21<03:48, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.2705078125, 'image_reward_oidp': 0.59326171875, 'image_reward_diop': 1.00390625, 'image_reward_didp': 0.9697265625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 288/3300 [00:21<04:20, 11.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.619140625, 'image_reward_oidp': 0.8466796875, 'image_reward_diop': 0.352294921875, 'image_reward_didp': 0.86962890625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 296/3300 [00:22<04:04, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_reward_oiop': 1.01953125, 'image_reward_oidp': 0.363525390625, 'image_reward_diop': 1.1728515625, 'image_reward_didp': 1.00390625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3300/3300 [02:25<00:00, 22.72it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ImageReward as RM\n",
    "model = RM.load(\"ImageReward-v1.0\")\n",
    "\n",
    "import torch\n",
    "def image_reward_reward(sample, i):\n",
    "    if i not in idx_of_interest:\n",
    "        return sample[\"image_reward\"]\n",
    "    images_part = [sample[\"image_original\"], sample[\"image_original\"], sample[\"image_distorted\"],  sample[\"image_distorted\"]]\n",
    "    prompts_part = [\n",
    "        sample[\"prompt_original\"],\n",
    "        sample[\"prompt_distorted\"],\n",
    "        sample[\"prompt_original\"],\n",
    "        sample[\"prompt_distorted\"]\n",
    "    ] \n",
    "    rewards = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            for image, prompt in zip(images_part, prompts_part):\n",
    "                reward = model.score(prompt, image)\n",
    "                rewards.append(reward)\n",
    "\n",
    "    results = {\n",
    "            \"image_reward_oiop\": float(rewards[0]), # original image, original prompt\n",
    "            \"image_reward_oidp\": float(rewards[1]), # original image, distorted prompt\n",
    "            \"image_reward_diop\": float(rewards[2]), # distorted image, original prompt\n",
    "            \"image_reward_didp\": float(rewards[3]), # distorted image, distorted prompt \n",
    "    }\n",
    "    print(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "rewards = []\n",
    "import tqdm\n",
    "for i, sample in enumerate(tqdm.tqdm(dataset)):\n",
    "    reward = image_reward_reward(sample, i) \n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb1e3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(rewards) == len(dataset[\"image_reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6709d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(\"image_reward\").add_column(\"image_reward\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "add25e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.push_to_hub(\"weathon/aas_benchmark_2\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c21e15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412b685594764eedab02d703774b856d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/15 shards):   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"aas_benchmark_2_with_blip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b07723fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../hpsv3_rewards.pkl\", \"rb\") as f:\n",
    "    hpsv3_rewards = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bff54b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(hpsv3_rewards) == len(dataset[\"hpsv3_reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b3dd627",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpsv3_rewards_2 = []\n",
    "for i in hpsv3_rewards:\n",
    "    hpsv3_rewards_2.append({\n",
    "        k:i.cpu().detach().tolist() if isinstance(i, torch.Tensor) else i\n",
    "        for k, i in i.items()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "061498da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(\"hpsv3_reward\").add_column(\"hpsv3_reward\", hpsv3_rewards_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e1f6a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4365bfdd96f54577b8bb8844fcd99a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/15 shards):   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"aas_benchmark_2_with_blip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
