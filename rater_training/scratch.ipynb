{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92505d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForImageTextRetrieval\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-large-coco\")\n",
    "model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-large-coco\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d1b516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"qkv\", \"query\", \"key\", \"value\", \"dense\", \"projection\", \"fc1\", \"fc2\", \"text_proj\", \"visual_proj\", \"position_embeddings\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c057c0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,041,920 || all params: 487,170,562 || trainable%: 8.4245\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5e96fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor(images=Image.new('RGB', (94, 34)), text=\"A cat\", return_tensors=\"pt\").pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.cuda()\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "train_ds = load_dataset(\"zai-org/VisionRewardDB-Image\", split='train[:40000]')\n",
    "test_ds = load_dataset(\"zai-org/VisionRewardDB-Image\", split='train[40000:]')\n",
    "\n",
    "import io, math, random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"rules.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "df['Dimension'] = df['Dimension'].ffill()\n",
    "\n",
    "df['dim_key'] = df['Dimension'].apply(lambda x: re.search(r'\\((.*?)\\)', x).group(1) if re.search(r'\\((.*?)\\)', x) else x)\n",
    "\n",
    "guide = {\n",
    "    dim_key: {\n",
    "        int(row['Score']): row['Option'] + \": \" +str(row['Description']).strip()\n",
    "        for _, row in group.iterrows()\n",
    "    }\n",
    "    for dim_key, group in df.groupby('dim_key')\n",
    "}\n",
    "\n",
    "dims = {k: v for k, v in guide.items() if k not in [\"unsafe type\", \"hands\", \"face\", \"body\", \"safety\", \"lighting aesthetic\", \"symmetry\"]}.keys()\n",
    "dims = list(dims)\n",
    "dim_min = {i:min(guide[i].keys()) for i in guide.keys()}\n",
    "\n",
    "# %%\n",
    "\n",
    "import json\n",
    "with open(\"prompts.json\", \"r\") as f:\n",
    "    prompt_dict = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d58562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_data(sample):\n",
    "    images = []\n",
    "    dims_selected = []\n",
    "    # print(len(sample[\"image\"]), len(sample[\"annotation\"]))\n",
    "    for image in range(len(sample['image'])):\n",
    "        images.append(sample['image'][image])\n",
    "        try:\n",
    "            if random.random()>0.5:\n",
    "                # sample a dim with score>=0 \n",
    "                dims_selected.append(random.choice(list([i for i in dims if sample['annotation'][image][i]>=0])))\n",
    "            else:\n",
    "                # sample a dim with score<0\n",
    "                dims_selected.append(random.choice(list([i for i in dims if sample['annotation'][image][i]<0])))\n",
    "        except IndexError:\n",
    "            dims_selected.append(random.choice(dims))\n",
    "            \n",
    "\n",
    "    prompts = [prompt_dict[dim] for i, dim in enumerate(dims_selected)]\n",
    "    images = list(sample['image'])\n",
    "    n_images = len(images)\n",
    "    n_prompts = len(prompts) \n",
    "    inputs = processor(images=images, text=prompts, return_tensors=\"pt\", padding=True)\n",
    "    answers = [1 if i[dim]<0 else (0.5 if i[dim]==0 else 0) for i, dim in zip(sample[\"annotation\"], dims_selected)]\n",
    "    labels = torch.tensor(answers)\n",
    "    inputs['labels'] = labels\n",
    "    inputs['dim'] = [dims.index(dim) for dim in dims_selected]\n",
    "    inputs['n_images'] = [n_images] * len(inputs['input_ids'])\n",
    "    return {\n",
    "        'pixel_values': inputs['pixel_values'],\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': inputs['labels'], \n",
    "        'dims': dims_selected,\n",
    "        'n_images': inputs['n_images'],\n",
    "        # \"annotation\": [i[dim] for i, dim in zip(sample[\"annotation\"], dims_selected)],\n",
    "    } \n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "train_ds = train_ds.with_transform(format_data)\n",
    "test_ds = test_ds.with_transform(format_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d31cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds[0:7][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc3d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "class Rater(PreTrainedModel):\n",
    "    def __init__(self, backbone):\n",
    "      super().__init__(PretrainedConfig())\n",
    "      self.backbone = backbone\n",
    "      self.head = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask, n_images, labels=None):\n",
    "      n_images = n_images[0]\n",
    "      outputs = self.backbone(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask)\n",
    "      itm_scores = self.head(outputs.question_embeds[:,0,:]).squeeze(-1)\n",
    "\n",
    "      if labels is not None:\n",
    "        assert itm_scores.shape == labels.shape, f\"{itm_scores.shape} {labels.shape}\"\n",
    "        assert itm_scores.shape[0] == n_images\n",
    "        bce_loss = torch.nn.functional.binary_cross_entropy_with_logits(itm_scores, labels)\n",
    "        mae_loss = torch.nn.functional.l1_loss(torch.sigmoid(itm_scores), labels)\n",
    "        loss = bce_loss + mae_loss\n",
    "\n",
    "        try: \n",
    "          wandb.log({\"bce_loss\": bce_loss, \"acc\": ((itm_scores>0) == (labels>0.5)).float().mean(), \"mae_loss\": mae_loss})\n",
    "        except:\n",
    "          pass\n",
    "        outputs['loss'] = loss\n",
    "\n",
    "      return outputs\n",
    "\n",
    "my_rater = Rater(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79635da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_rater = my_rater.cpu()\n",
    "# with torch.no_grad():\n",
    "#     my_rater(**train_ds[0:2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972cc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,585,219 || all params: 458,714,630 || trainable%: 2.7436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wg25r/miniconda/envs/neg/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/wg25r/miniconda/envs/neg/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwguo6358\u001b[0m (\u001b[33m3dsmile\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wg25r/itm/AAS/rater_training/wandb/run-20251017_225655-b273je60</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/3dsmile/huggingface/runs/b273je60' target=\"_blank\">curious-puddle-428</a></strong> to <a href='https://wandb.ai/3dsmile/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/3dsmile/huggingface' target=\"_blank\">https://wandb.ai/3dsmile/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/3dsmile/huggingface/runs/b273je60' target=\"_blank\">https://wandb.ai/3dsmile/huggingface/runs/b273je60</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='172' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  172/25000 03:15 < 7:55:31, 0.87 it/s, Epoch 0.14/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"BLIP-Reward-Long\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.001,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    max_grad_norm=1.0,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=min(os.cpu_count(), 16),\n",
    "    fp16=True,\n",
    "    warmup_ratio=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # lr_scheduler_kwargs={\"num_decay_steps\": 500},\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"qkv\", \"query\", \"key\", \"value\", \"dense\", \"projection\"],\n",
    "    modules_to_save=[\"head\"] \n",
    ")\n",
    "my_rater = get_peft_model(my_rater, lora_config)  \n",
    "my_rater = my_rater.to(\"cuda\")\n",
    "my_rater.print_trainable_parameters()\n",
    "from transformers import Trainer \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=my_rater,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    processing_class=processor,\n",
    ")\n",
    "\n",
    "# %%\n",
    "trainer.train() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
