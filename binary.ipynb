{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2982ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb711d4e4087427fb283017f98283c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForImageTextRetrieval\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
    "model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
    "\n",
    "model = model.cuda()\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "train_ds = load_dataset(\"zai-org/VisionRewardDB-Image\", split='train[:40000]')\n",
    "test_ds = load_dataset(\"zai-org/VisionRewardDB-Image\", split='train[40000:]')\n",
    "\n",
    "import io, math, random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"rules.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "df['Dimension'] = df['Dimension'].ffill()\n",
    "\n",
    "df['dim_key'] = df['Dimension'].apply(lambda x: re.search(r'\\((.*?)\\)', x).group(1) if re.search(r'\\((.*?)\\)', x) else x)\n",
    "\n",
    "guide = {\n",
    "    dim_key: {\n",
    "        int(row['Score']): row['Option'] + \": \" +str(row['Description']).strip()\n",
    "        for _, row in group.iterrows()\n",
    "    }\n",
    "    for dim_key, group in df.groupby('dim_key')\n",
    "}\n",
    "\n",
    "dims = {k: v for k, v in guide.items() if k not in [\"unsafe type\", \"hands\", \"face\", \"body\", \"safety\", \"lighting aesthetic\"]}.keys()\n",
    "dims = list(dims)\n",
    "dim_min = {i:min(guide[i].keys()) for i in guide.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b47a79e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"prompts.json\", \"r\") as f:\n",
    "    prompt_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53a5bf7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'background': \"The images's background is bad, there is no background or the background is ugly\",\n",
       " 'clarity': \"The images's clarity is blurry, the image is noticeably blurry, as though noise or distortion is present.\",\n",
       " 'color aesthetic': \"The images's color aesthetic is bad, there are ugly colors, the colors are unpleasant, often due to lack of harmony, abruptness, overexposure, or poor color choice.\",\n",
       " 'color brightness': \"The images's color brightness is dark: Purely judging the color, it is dark.\",\n",
       " 'detail realism': \"The images's detail realism is bad, inauthentic; The image is inconsistent with reality, featuring heavy object distortion or deformation, even when scaled down, the flaws in realism are apparent.\",\n",
       " 'detail refinement': \"Important elements appear noticeably rough on closer inspection, or the overall image feels rough. Important details are blatantly rough or unfinished. So rough that it is impossible to discern objects in the image. The image isn't just indecipherable, but it also appears extremely fragmented/broken.\",\n",
       " 'emotion': \"The images's shows negative emotions. The image invokes negative emotions, such as creepiness, eeriness, or hostility. The image elicits negative emotions, such as horror or disgust.\",\n",
       " 'lighting distinction': \"The images's lighting distinction is bad: No light and shadow: No obvious light effects or shadows are seen.\",\n",
       " 'main object': \"The images's main object is bad: Not prominent: There is no main object, or the main object is too inconspicuous (small + peripheral + unnoticeable).\",\n",
       " 'object pairing': \"The images's object pairing is bad: Disharmonious: Includes any noticeable mismatch in harmony, proportion, or aesthetics, such as awkward pairings, chaotic or discordant compositions, etc.\",\n",
       " 'richness': \"The images's richness is bad: Empty: The image appears to lack clear intent or design and seems random or completely unfinished. \",\n",
       " 'symmetry': \"The images's symmetry is bad: Asymmetrical: Choose asymmetrical only if the following conditions are met:1. The object is naturally symmetrical in real life but appears clearly asymmetrical in the image.2. The object itself is asymmetrical due to reasons other than perspective.\"}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ceede1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    images = []\n",
    "    dims_selected = []\n",
    "    # print(len(sample[\"image\"]), len(sample[\"annotation\"]))\n",
    "    for image in range(len(sample['image'])):\n",
    "        images.append(sample['image'][image])\n",
    "        try:\n",
    "            if random.random()>0.5:\n",
    "                # sample a dim with score>=0 \n",
    "                dims_selected.append(random.choice(list([i for i in dims if sample['annotation'][image][i]>=0])))\n",
    "            else:\n",
    "                # sample a dim with score<0\n",
    "                dims_selected.append(random.choice(list([i for i in dims if sample['annotation'][image][i]<0])))\n",
    "        except IndexError:\n",
    "            dims_selected.append(random.choice(dims))\n",
    "    \n",
    "    prompts = [prompt_dict[dim] for i, dim in enumerate(dims_selected)]\n",
    "    images = list(sample['image'])\n",
    "    n_images = len(images)\n",
    "    n_prompts = len(prompts) \n",
    "    # prompts = [[prompt] * len(images) for prompt in prompts] # n prompt x n image\n",
    "    # prompts = [item for sublist in prompts for item in sublist]\n",
    "    # images = images * n_prompts # n prompt x n images\n",
    "    inputs = processor(images=images, text=prompts, return_tensors=\"pt\", padding=True)\n",
    "    answers = [1 if i[dim]<0 else 0 for i, dim in zip(sample[\"annotation\"], dims_selected)]\n",
    "    # answers = [(-np.sign(i[dim])+1)/2 for i in sample[\"annotation\"]] \n",
    "    answers = [(1-i, i) for i in answers]\n",
    "    labels = torch.tensor(answers)\n",
    "    inputs['labels'] = labels\n",
    "    inputs['n_images'] = [n_images] * len(inputs['input_ids'])\n",
    "    return {\n",
    "        'pixel_values': inputs['pixel_values'],\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': inputs['labels'], \n",
    "        'dims': dims_selected,\n",
    "        'n_images': inputs['n_images'],\n",
    "        \"prompts\": prompts,\n",
    "        \"annotation\": [i[dim] for i, dim in zip(sample[\"annotation\"], dims_selected)],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e58cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_items = [sorted(i.items()) for i in [guide[dim] for dim in dims]]\n",
    "\n",
    "# import json\n",
    "# with open(\"prompts.json\", \"w\") as f:\n",
    "#     json.dump([f\"The images's {dim} is bad: {sorted_items[i][0][1]}\" for i, dim in enumerate(dims)], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5b1cb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['background',\n",
       " 'clarity',\n",
       " 'color aesthetic',\n",
       " 'color brightness',\n",
       " 'detail realism',\n",
       " 'detail refinement',\n",
       " 'emotion',\n",
       " 'lighting distinction',\n",
       " 'main object',\n",
       " 'object pairing',\n",
       " 'richness',\n",
       " 'symmetry']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{dim}\" for i, dim in enumerate(dims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3fc674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.with_transform(format_data)\n",
    "test_ds = test_ds.with_transform(format_data)\n",
    "\n",
    "inputs = train_ds[:18]  \n",
    "# print(inputs[\"labels\"])\n",
    "# [len(i) for i in train_ds[:8].values()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "404192ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The images's shows negative emotions. The image invokes negative emotions, such as creepiness, eeriness, or hostility. The image elicits negative emotions, such as horror or disgust.\",\n",
       " \"The images's symmetry is bad: Asymmetrical: Choose asymmetrical only if the following conditions are met:1. The object is naturally symmetrical in real life but appears clearly asymmetrical in the image.2. The object itself is asymmetrical due to reasons other than perspective.\",\n",
       " \"The images's symmetry is bad: Asymmetrical: Choose asymmetrical only if the following conditions are met:1. The object is naturally symmetrical in real life but appears clearly asymmetrical in the image.2. The object itself is asymmetrical due to reasons other than perspective.\",\n",
       " \"The images's color brightness is dark: Purely judging the color, it is dark.\",\n",
       " \"The images's symmetry is bad: Asymmetrical: Choose asymmetrical only if the following conditions are met:1. The object is naturally symmetrical in real life but appears clearly asymmetrical in the image.2. The object itself is asymmetrical due to reasons other than perspective.\",\n",
       " \"The images's clarity is blurry, the image is noticeably blurry, as though noise or distortion is present.\",\n",
       " \"The images's background is bad, there is no background or the background is ugly\",\n",
       " \"The images's color aesthetic is bad, there are ugly colors, the colors are unpleasant, often due to lack of harmony, abruptness, overexposure, or poor color choice.\",\n",
       " \"Important elements appear noticeably rough on closer inspection, or the overall image feels rough. Important details are blatantly rough or unfinished. So rough that it is impossible to discern objects in the image. The image isn't just indecipherable, but it also appears extremely fragmented/broken.\",\n",
       " \"The images's clarity is blurry, the image is noticeably blurry, as though noise or distortion is present.\",\n",
       " \"Important elements appear noticeably rough on closer inspection, or the overall image feels rough. Important details are blatantly rough or unfinished. So rough that it is impossible to discern objects in the image. The image isn't just indecipherable, but it also appears extremely fragmented/broken.\",\n",
       " \"Important elements appear noticeably rough on closer inspection, or the overall image feels rough. Important details are blatantly rough or unfinished. So rough that it is impossible to discern objects in the image. The image isn't just indecipherable, but it also appears extremely fragmented/broken.\",\n",
       " \"The images's color aesthetic is bad, there are ugly colors, the colors are unpleasant, often due to lack of harmony, abruptness, overexposure, or poor color choice.\",\n",
       " \"The images's background is bad, there is no background or the background is ugly\",\n",
       " \"The images's background is bad, there is no background or the background is ugly\",\n",
       " \"The images's lighting distinction is bad: No light and shadow: No obvious light effects or shadows are seen.\",\n",
       " \"The images's lighting distinction is bad: No light and shadow: No obvious light effects or shadows are seen.\",\n",
       " \"The images's lighting distinction is bad: No light and shadow: No obvious light effects or shadows are seen.\"]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"prompts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bbe68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inputs[\"annotation\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ce3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds[:10][\"labels\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab64925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(i) for i in train_ds[:8].values()]\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "class Rater(PreTrainedModel):\n",
    "    def __init__(self, backbone):\n",
    "      super().__init__(PretrainedConfig())\n",
    "      self.backbone = backbone\n",
    "      # self.t_score = 0.2\n",
    "      # self.t_ce = 0.2\n",
    "      self.t = torch.nn.Parameter(torch.tensor(0.2))\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask, dim, n_images, labels=None):\n",
    "      dim = dim[0]\n",
    "      n_images = n_images[0]\n",
    "      outputs = self.backbone(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask)\n",
    "      itm_scores = outputs[0]\n",
    "\n",
    "      if labels is not None:\n",
    "        assert itm_scores.shape[0] == labels.shape[0] == n_images, f\"{itm_scores.shape[0]} {labels.shape[0]} {n_images}\"\n",
    "        assert itm_scores.shape[1] == labels.shape[1] == 2\n",
    "        bce_loss = torch.nn.functional.cross_entropy(itm_scores, labels)\n",
    "        loss = bce_loss\n",
    "\n",
    "        assert itm_scores.argmax(-1).shape == labels.argmax(-1).shape\n",
    "        try:\n",
    "          wandb.log({\"bce_loss\": bce_loss, \"acc\": (itm_scores.argmax(-1) == labels.argmax(-1)).float().mean()})\n",
    "        except:\n",
    "          pass\n",
    "        outputs['loss'] = loss\n",
    "\n",
    "      return outputs\n",
    "\n",
    "my_rater = Rater(model).cuda()\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"BLIP-Reward\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.0001,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    max_grad_norm=1.0,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=os.cpu_count(),\n",
    "    fp16=True,\n",
    "    warmup_ratio=0.01,\n",
    "    lr_scheduler_type=\"warmup_stable_decay\",\n",
    "    lr_scheduler_kwargs={\"num_decay_steps\": 500},\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=my_rater,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    processing_class=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c7222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwguo6358\u001b[0m (\u001b[33m3dsmile\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wg25r/itm/wandb/run-20251009_193544-6evlh4i9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/3dsmile/huggingface/runs/6evlh4i9' target=\"_blank\">BLIP-Reward</a></strong> to <a href='https://wandb.ai/3dsmile/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/3dsmile/huggingface' target=\"_blank\">https://wandb.ai/3dsmile/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/3dsmile/huggingface/runs/6evlh4i9' target=\"_blank\">https://wandb.ai/3dsmile/huggingface/runs/6evlh4i9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 6464 64\n",
      "\n",
      "646464 64 \n",
      "64 6464\n",
      " \n",
      "646464646464\n",
      "     6464646464\n",
      " \n",
      "\n",
      "6464 64\n",
      "\n",
      "64\n",
      "6464646464\n",
      " 6464646464   \n",
      " 64 6464 64646464    6464\n",
      "646464646464 \n",
      "\n",
      " 646464\n",
      "64 \n",
      "\n",
      " 64\n",
      "\n",
      "64 \n",
      "\n",
      "  6464 646464646464 64\n",
      "  64 64\n",
      "64\n",
      "64\n",
      "646464\n",
      "6464\n",
      "6464  64\n",
      "\n",
      " 646464646464  \n",
      " 64\n",
      " 64\n",
      " 64646464\n",
      "646464\n",
      "6464\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " 646464  64 64 64646464 64 64\n",
      " 64 6464 64646464\n",
      "\n",
      "64 64 64\n",
      " 6464\n",
      " 646464\n",
      " \n",
      "64\n",
      " \n",
      "64 \n",
      "  \n",
      "\n",
      " \n",
      " 646464\n",
      "64 \n",
      " 6464 64646464 6464646464 \n",
      "64 \n",
      "\n",
      "\n",
      "\n",
      " 64646464\n",
      "\n",
      "64646464\n",
      " 646464 \n",
      "\n",
      "64\n",
      "64\n",
      "\n",
      "  6464\n",
      " \n",
      "6464\n",
      "6464 \n",
      "6464 64\n",
      "64\n",
      "64\n",
      "64 64 64\n",
      "   6464646464 6464 \n",
      " 64\n",
      "\n",
      "\n",
      "6464\n",
      "64 \n",
      "\n",
      "6464 64 64\n",
      "\n",
      " 6464\n",
      "64\n",
      "\n",
      "\n",
      "64\n",
      "64  6464\n",
      "\n",
      "6464 64  64 64646464 64\n",
      " 6464\n",
      "\n",
      "64\n",
      "6464  \n",
      "6464\n",
      "64\n",
      "64  6464\n",
      "\n",
      "64\n",
      " 6464\n",
      " 6464 \n",
      "6464 64\n",
      "64\n",
      " 64 646464\n",
      "\n",
      " 64\n",
      "64 64\n"
     ]
    }
   ],
   "source": [
    "trainer.train() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
