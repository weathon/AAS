{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99e68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_paths = [\"weathon/aas_benchmark-nano-banana\",\"weathon/aas_benchmark-stable_diffusion_xl\",\"weathon/aas_benchmark-flux_krea\",\"weathon/aas_benchmark-flux_dev\",\"weathon/aas_benchmark-grpo_flux\",\"weathon/aas_benchmark-playground\",\"weathon/aas_benchmark-sd3_medium_grpo\",\"weathon/aas_benchmark-sd3_medium_grpo_geneval\",\"weathon/aas_benchmark-stable_diffusion_3.5_medium\",\"weathon/aas_benchmark-sd3_medium_grpo\",\"weathon/aas_benchmark-dance_flux\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce33c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [load_dataset(path, split=\"train\") for path in dataset_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc79425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "dataset = concatenate_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897e56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pydantic import BaseModel\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from typing import Literal\n",
    "from openai import OpenAI \n",
    "\n",
    "\n",
    "class JudgeResponse(BaseModel):\n",
    "    reasoning: str\n",
    "    main_concepts: int\n",
    "    special_effects: int\n",
    "\n",
    "\n",
    "def encode_image(image: Image.Image) -> str:\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def ensure_percentage(value: int) -> None:\n",
    "    if not (0 <= value <= 100):\n",
    "        raise ValueError(f\"LLM returned value outside 0-100 range: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6dccda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "deepinfra_api_key = os.getenv(\"DEEPNFRA_API_KEY\")\n",
    "openai = OpenAI(\n",
    "    api_key=deepinfra_api_key,\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51c18fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(image: Image.Image, original_prompt: str, distorted_prompt: str) -> JudgeResponse:\n",
    "    encoded = encode_image(image)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a strict image judge. Reply with a JSON object that matches the provided schema.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        f\"Original prompt:\\n{original_prompt}\\n\\n\"\n",
    "                        f\"Distorted prompt:\\n{distorted_prompt}\\n Answer the following using integers between 0 and 100 inclusive. IMPORTANT: If visual effects, styles, or distortions make the main concept harder to see but it is still present, DO NOT decrease the main concept score. 1. Main concept (0-100): score how clearly the main subjects or scenes from the ORIGINAL prompt appear, regardless of added effects that may partially obscure them. 2. Special effects (0-100): score how well the stylistic details, modifiers, and effects described in the distorted prompt appear. The rating should not be binary (just 0 or 100) but smooth, only rate 100 if it follows all effetcs and only rate 0 if none of the effects are visible. To do list for the effects, list the main effects mentioned in the distorted prompt and check if each of them is present, then rate based on the fraction of effects that are present. You should do the thinking in the reasoning part, list all effects mentioned in the distorted prompt, and see if the effects are in the image.\" \n",
    "                    ),\n",
    "                },\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{encoded}\"}},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            response = openai.chat.completions.parse(\n",
    "                model=\"Qwen/Qwen3-VL-30B-A3B-Instruct\",\n",
    "                messages=messages,\n",
    "                response_format=JudgeResponse,\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            parsed = response.choices[0].message.parsed\n",
    "            ensure_percentage(parsed.main_concepts)\n",
    "            ensure_percentage(parsed.special_effects)\n",
    "            return parsed\n",
    "        except Exception as exc:\n",
    "            if attempt == 4:\n",
    "                raise\n",
    "            print(f\"retrying llm judge due to error: {exc}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcfb7d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_original', 'image_distorted', 'index', 'prompt_original', 'prompt_distorted', 'selected_dims', 'hpsv2', 'llm_judge', 'model'],\n",
       "    num_rows: 3300\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4da5d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    original_judge = judge(example[\"image_original\"], example[\"prompt_original\"], example[\"prompt_distorted\"])\n",
    "    distorted_judge = judge(example[\"image_distorted\"], example[\"prompt_original\"], example[\"prompt_distorted\"])\n",
    "\n",
    "    example[\"llm_judge\"] = {\n",
    "            \"llm_original_reasoning\": original_judge.reasoning,\n",
    "            \"llm_original_main_concepts\": original_judge.main_concepts,\n",
    "            \"llm_original_special_effects\": original_judge.special_effects,\n",
    "            \"llm_distorted_reasoning\": distorted_judge.reasoning,\n",
    "            \"llm_distorted_main_concepts\": distorted_judge.main_concepts,\n",
    "            \"llm_distorted_special_effects\": distorted_judge.special_effects,\n",
    "        }\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c82c1b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57650d97a95f44119f972f0be936aa6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=1374, total_tokens=17758, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.01661862)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=1338, total_tokens=17722, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.01660818)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=1347, total_tokens=17731, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.01661079)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=1348, total_tokens=17732, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.016611079999999997)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=570, total_tokens=16954, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.016385459999999998)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=1350, total_tokens=17734, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.01661166)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=1345, total_tokens=17729, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.01661021)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=586, total_tokens=16970, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.016390099999999998)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=573, total_tokens=16957, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.01638633)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=1339, total_tokens=17723, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.01660847)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=1340, total_tokens=17724, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.016608759999999997)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=1337, total_tokens=17721, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.01660789)\n",
      "retrying llm judge due to error: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=16384, prompt_tokens=1365, total_tokens=17749, completion_tokens_details=None, prompt_tokens_details=None, estimated_cost=0.01661601)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(process_example, batched=False, num_proc=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d70c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"weathon/aas_benchmark\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947f172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"weathon/aas_benchmark\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f2c1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Attention is not installed. Falling to SDPA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4780223a94b4028a3c98b92ef0b7d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2VLRewardModelBT were not initialized from the model checkpoint at Qwen/Qwen2-VL-7B-Instruct and are newly initialized: ['rm_head.0.bias', 'rm_head.0.weight', 'rm_head.3.bias', 'rm_head.3.weight', 'rm_head.5.bias', 'rm_head.5.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from hpsv3 import HPSv3RewardInferencer\n",
    "\n",
    "inferencer = HPSv3RewardInferencer(device='cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ce0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def hpsv3_reward(sample):\n",
    "    images = []\n",
    "    prompts = [] \n",
    "    images_part = [sample[\"image_original\"], sample[\"image_original\"], sample[\"image_distorted\"],  sample[\"image_distorted\"]]\n",
    "    prompts_part = [\n",
    "        sample[\"prompt_original\"],\n",
    "        sample[\"prompt_distorted\"],\n",
    "        sample[\"prompt_original\"],\n",
    "        sample[\"prompt_distorted\"]\n",
    "    ] \n",
    "    images.extend(images_part)\n",
    "    prompts.extend(prompts_part)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rewards = inferencer.reward(prompts=prompts, image_paths=images)\n",
    "    results = []\n",
    "    for i in range(0, len(rewards), 4):\n",
    "        results.append({\n",
    "            \"hpsv3_oiop\": rewards[i], # original image, original prompt\n",
    "            \"hpsv3_oidp\": rewards[i+1], # original image, distorted prompt\n",
    "            \"hpsv3_diop\": rewards[i+2], # distorted image, original prompt\n",
    "            \"hpsv3_didp\": rewards[i+3], # distorted image, distorted prompt \n",
    "        })\n",
    "    sample[\"hpsv3_reward\"] = results\n",
    "    return sample\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c4e275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_original': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1024x1024>,\n",
       " 'image_distorted': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1024x1024>,\n",
       " 'index': 0,\n",
       " 'prompt_original': 'A standing toilet in a bathroom stall next to a trash can.',\n",
       " 'prompt_distorted': 'A standing toilet in a bathroom stall beside a trash can, rendered with no light or shadow, appearing random and unfinished, lacking clear intent or design, with flat, indistinct surfaces and no depth or texture.',\n",
       " 'selected_dims': '[\"richness\", \"lighting distinction\"]',\n",
       " 'hpsv2': {'distorted': 0.0,\n",
       "  'distorted_original_prompt': 0.0,\n",
       "  'original': 0.0,\n",
       "  'original_distorted_prompt': 0.0},\n",
       " 'llm_judge': {'llm_distorted_main_concepts': 100,\n",
       "  'llm_distorted_reasoning': 'The original prompt describes a standing toilet in a bathroom stall next to a trash can. The distorted prompt adds stylistic effects: no light or shadow, random and unfinished appearance, lack of clear intent or design, flat and indistinct surfaces, and no depth or texture.\\n\\nEffects to check:\\n1. No light or shadow: The image has no shading or shadows, consistent with this effect.\\n2. Appearing random and unfinished: The drawing style is simple and lacks detail, giving an unfinished look.\\n3. Lacking clear intent or design: The image is simplistic and lacks artistic refinement, matching this description.\\n4. Flat, indistinct surfaces: The surfaces are rendered without texture or depth, appearing flat.\\n5. No depth or texture: The image lacks depth perception and textural detail.\\n\\nAll five effects mentioned in the distorted prompt are present in the image. Therefore, the special effects score is 100.\\n\\nThe main concept (toilet in a bathroom stall next to a trash can) is clearly visible despite the stylistic choices. The subjects are identifiable and correctly positioned. Therefore, the main concept score is 100.',\n",
       "  'llm_distorted_special_effects': 100,\n",
       "  'llm_original_main_concepts': 100,\n",
       "  'llm_original_reasoning': \"The distorted prompt describes a scene with several specific stylistic effects: \\n\\n1. No light or shadow\\n2. Appearing random and unfinished\\n3. Lacking clear intent or design\\n4. Flat, indistinct surfaces\\n5. No depth or texture\\n\\nLooking at the image:\\n\\n- The image clearly shows a standing toilet in a bathroom stall next to a trash can, so the main concept is present.\\n\\n- However, the image does not match the described distortions. It has clear lighting and shadows, with a clean, designed appearance.\\n\\n- The surfaces are not flat or indistinct; they have texture (hexagonal tiles, metallic trash can).\\n\\n- The image has depth and clear design intent.\\n\\n- The scene is not random or unfinished; it's a clean, well-lit bathroom.\\n\\nAll five effects mentioned in the distorted prompt are absent from the image. Therefore, the special effects score is 0, as none of the described effects are present.\\n\\nThe main concept is clearly visible despite the lack of distortions, so the main concept score remains high.\",\n",
       "  'llm_original_special_effects': 0},\n",
       " 'model': 'nano-banana',\n",
       " 'hpsv3_reward': [{'hpsv3_oiop': tensor([10.1473, -5.0119], device='cuda:2'),\n",
       "   'hpsv3_oidp': tensor([11.2330, -5.0555], device='cuda:2'),\n",
       "   'hpsv3_diop': tensor([ 2.4968, -4.1009], device='cuda:2'),\n",
       "   'hpsv3_didp': tensor([ 8.6927, -4.9227], device='cuda:2')}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpsv3_reward(dataset[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee1055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b1517d5e0047909aaaf18d6e942c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(hpsv3_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
