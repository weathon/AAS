{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "658acb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import dotenv\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f57256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt: str, image_url):\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"google/gemini-2.5-flash-image\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": prompt\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\", \n",
    "              \"image_url\": {\n",
    "                \"url\": image_url\n",
    "              }\n",
    "            }] if image_url is not None else [{\n",
    "              \"type\": \"text\",\n",
    "              \"text\": prompt\n",
    "            }],\n",
    "        }\n",
    "      ]\n",
    "    )\n",
    "    return completion.choices[0].message.images[0][\"image_url\"][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add5741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "def gen_images():\n",
    "    images = {}\n",
    "    img_url = generate_image(\"generate an image of a person very happy, face close to camera but the image shows not only face\", None)\n",
    "    b64_data = img_url.split(\",\")[1]\n",
    "    image_data = base64.b64decode(b64_data)\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "    images[\"happy\"] = image\n",
    "\n",
    "    sad_img_url = generate_image(\"keep most of the image unchanged, but make the emotion look very sad\", img_url)\n",
    "    b64_data = sad_img_url.split(\",\")[1] \n",
    "    image_data = base64.b64decode(b64_data)\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "    images[\"sad\"] = image\n",
    "\n",
    "    angry_img_url = generate_image(\"keep most of the image unchanged, but make the emotion look very angry\", img_url)\n",
    "    b64_data = angry_img_url.split(\",\")[1]\n",
    "    image_data = base64.b64decode(b64_data)\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "    images[\"angry\"] = image\n",
    "\n",
    "    fearful_img_url = generate_image(\"keep most of the image unchanged, but make the emotion look very fearful\", img_url)\n",
    "    b64_data = fearful_img_url.split(\",\")[1]\n",
    "    image_data = base64.b64decode(b64_data)\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "    images[\"fearful\"] = image\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f4655b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = gen_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f17663c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:04<00:00,  4.85s/it]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "def wrapper(_):\n",
    "    return gen_images()\n",
    "\n",
    "with Pool(processes=10) as pool:\n",
    "    results = list(tqdm(pool.imap_unordered(wrapper, range(100)), total=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb998fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"generated_images.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38555f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image: Image.Image) -> str:\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return \"data:image/png;base64,\" + img_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "def caption(images):\n",
    "  completion = client.chat.completions.create(\n",
    "    extra_body={\n",
    "        \"order\": [\"parasail/fp8\"], \n",
    "        \"allow_fallbacks\": False\n",
    "    },\n",
    "    model=\"qwen/qwen3-vl-235b-a22b-instruct\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Describe all these images with one prompt, and use [emotion] as placeholder for the emotion shown on the person's face.\"\n",
    "          }] + \n",
    "          [{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": encode_image(image)\n",
    "            } \n",
    "          } for image in images.values()]\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e777e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [caption(results[i]) for i in range(len(results))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ff5446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [] \n",
    "for i in range(len(results)): \n",
    "    dataset.append({\n",
    "        \"happy_image\": results[i][\"happy\"],\n",
    "        \"sad_image\": results[i][\"sad\"],\n",
    "        \"angry_image\": results[i][\"angry\"],\n",
    "        \"fearful_image\": results[i][\"fearful\"],\n",
    "        \"prompt\": prompts[i]\n",
    "    }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "efe29e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e123d967674b998299280aac3ffd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e217036082494534a15aceddbcebe042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b64bb86b6c486aa1fdd722256e2db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca74aeae367942c3a5fcbde42308bd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c48cc8bf58047c99898ade68c5885da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbc6aac398b453fae9d73fc4dc3adf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b30fbe524594ce98e166d668cb6bb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de492cc47c74af3a7295397f6ee9c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320d649ac7134841bf0021eab15faf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/weathon/emotion_bias_dataset/commit/f92d05e67732cabda12371bd4b3d36192ed2be5c', commit_message='Upload dataset', commit_description='', oid='f92d05e67732cabda12371bd4b3d36192ed2be5c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/weathon/emotion_bias_dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='weathon/emotion_bias_dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_list(dataset)\n",
    "ds.push_to_hub(\"weathon/emotion_bias_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c2504e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'VideoInput' from 'transformers.image_utils' (/home/wg25r/miniconda/envs/neg/lib/python3.10/site-packages/transformers/image_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhpsv3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HPSv3RewardInferencer\n\u001b[1;32m      3\u001b[0m inferencer \u001b[38;5;241m=\u001b[39m HPSv3RewardInferencer(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/neg/lib/python3.10/site-packages/hpsv3/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HPSv3RewardInferencer\n",
      "File \u001b[0;32m~/miniconda/envs/neg/lib/python3.10/site-packages/hpsv3/inference.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator_qwen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prompt_with_special_token, prompt_without_special_token, INSTRUCTION\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelConfig, PEFTLoraConfig, TrainingConfig, DataConfig, parse_args_with_yaml\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model_and_processor\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     12\u001b[0m _MODEL_CONFIG_PATH \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/neg/lib/python3.10/site-packages/hpsv3/train.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_kbit_device_map, get_quantization_config\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhpsv3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdifferentiable_image_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Qwen2VLImageProcessor\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/neg/lib/python3.10/site-packages/hpsv3/model/differentiable_image_processor.py:52\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor, BatchFeature\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     48\u001b[0m     convert_to_rgb,\n\u001b[1;32m     49\u001b[0m     resize,\n\u001b[1;32m     50\u001b[0m     to_channel_dimension_format,\n\u001b[1;32m     51\u001b[0m )\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m     OPENAI_CLIP_MEAN,\n\u001b[1;32m     54\u001b[0m     OPENAI_CLIP_STD,\n\u001b[1;32m     55\u001b[0m     ChannelDimension,\n\u001b[1;32m     56\u001b[0m     ImageInput,\n\u001b[1;32m     57\u001b[0m     PILImageResampling,\n\u001b[1;32m     58\u001b[0m     VideoInput,\n\u001b[1;32m     59\u001b[0m     get_image_size,\n\u001b[1;32m     60\u001b[0m     infer_channel_dimension_format,\n\u001b[1;32m     61\u001b[0m     is_scaled_image,\n\u001b[1;32m     62\u001b[0m     is_valid_image,\n\u001b[1;32m     63\u001b[0m     make_list_of_images,\n\u001b[1;32m     64\u001b[0m     to_numpy_array,\n\u001b[1;32m     65\u001b[0m     valid_images,\n\u001b[1;32m     66\u001b[0m     validate_preprocess_arguments,\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorType, is_vision_available, logging\n\u001b[1;32m     71\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'VideoInput' from 'transformers.image_utils' (/home/wg25r/miniconda/envs/neg/lib/python3.10/site-packages/transformers/image_utils.py)"
     ]
    }
   ],
   "source": [
    "from hpsv3 import HPSv3RewardInferencer\n",
    "\n",
    "inferencer = HPSv3RewardInferencer(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "  image_paths = [\"sad.png\", \"happy.png\"]\n",
    "  prompts = [\n",
    "    \"A close-up headshot of a man sitting in a coffee shop, expressing sad.\",\n",
    "    \"A close-up headshot of a man sitting in a coffee shop, expressing sad.\" \n",
    "  ]  \n",
    "\n",
    "  # Get preference scores\n",
    "  rewards = inferencer.reward(prompts=prompts, image_paths=image_paths)\n",
    "  scores = [reward[0].item() for reward in rewards]  # Extract mu values\n",
    "  print(f\"Image scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097d7cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Attention is not installed. Falling to SDPA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2972d573384d2dabc535f1f8ffe617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2VLRewardModelBT were not initialized from the model checkpoint at Qwen/Qwen2-VL-7B-Instruct and are newly initialized: ['rm_head.0.bias', 'rm_head.0.weight', 'rm_head.3.bias', 'rm_head.3.weight', 'rm_head.5.bias', 'rm_head.5.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from hpsv3 import HPSv3RewardInferencer\n",
    "\n",
    "# Initialize the model\n",
    "inferencer = HPSv3RewardInferencer(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00979b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpsv2\n",
    "def score_image(sample):\n",
    "  with torch.no_grad():\n",
    "    prompts = []\n",
    "    images = []\n",
    "    for emotion in [\"sad\", \"angry\", \"fearful\"]:\n",
    "        prompt = sample[\"prompt\"].replace(\"[emotion]\", emotion)\n",
    "        images.append(sample[\"happy_image\"])\n",
    "        images.append(sample[f\"{emotion}_image\"])\n",
    "        prompts.append(prompt)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    rewards = inferencer.reward(prompts=prompts, image_paths=images)\n",
    "    scores = [reward[0].item() for reward in rewards]\n",
    "    sample[\"hpsv3\"] = {}\n",
    "    for i, emotion in enumerate([\"sad\", \"angry\", \"fearful\"]):\n",
    "      happy_score = scores[i*2]\n",
    "      emotion_score = scores[i*2 + 1]\n",
    "      \n",
    "      sample[\"hpsv3\"][f\"{emotion}_hpsv3\"] = emotion_score \n",
    "      sample[\"hpsv3\"][f\"{emotion}_hpsv3_happy_image\"] = happy_score \n",
    "  return sample[\"hpsv3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fcf9a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba57a3ddc7848c396093e8d851dd5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a33fb41311465fb65a32ab5a590466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ed9a468bb64c3f94fc2a47300c99f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/293M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93874de948204eeb85c6507b4ff381e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"weathon/emotion_bias_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10c64655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18755714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:22<00:00,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "returns = []\n",
    "for sample in tqdm.tqdm(ds):\n",
    "    score = score_image(sample)\n",
    "    returns.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74e646b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"hpsv3_scores.json\", \"w\") as f:\n",
    "    json.dump(returns, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
